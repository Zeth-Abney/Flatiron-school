{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 29 Topic Review - Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes Function\n",
    "\" it allows you to develop a belief network taking into account all of the available information regarding the scenario\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bayesian formula for probability conditioned on *multiple* independent events is defined as follows:  \n",
    "$ \\Large P(y|x_1, x_2, ..., x_n) = \\frac{P(y)\\prod_{i}^{n}P(x_i|y)}{P(x_1, x_2, ..., x_n)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Preprocess data (clean data, split train-test samples) \n",
    "```Python \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('filepath.csv')\n",
    "\n",
    "y = df['target']\n",
    "X = df.drop('target',axis=1)\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Calculate $\\mu$ and $\\sigma$ for each feature. \n",
    "\n",
    "```Python\n",
    "train_df = pd.concat([X_train,y_train],axis=1)\n",
    "train_agg_df = train_df.groupby('target').agg(['mean','std'])\n",
    "train_agg_df\n",
    "```  \n",
    "<img src='images/agg_table.png'>  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Calculate conditional probability point estimates for each feature per target class. . .    \n",
    "$$ \\large \\text{point estimate for i'th feature} = P(x_i|y) = \\frac{1}{\\sqrt{2 \\pi \\sigma_i^2}}e^{\\frac{-(x-\\mu_i)^2}{2\\sigma_i^2}}$$ \n",
    "\n",
    "```Python\n",
    "def p_x_given_class(data:pd.DataFrame, feature:str, obs_idx:int, class_=0):\n",
    "    headers = list(data.columns)\n",
    "    headers.remove(feature)\n",
    "    agg_df = data.groupby(feature).agg(['mean','std'])\n",
    "\n",
    "    mu = train_agg_df[feature]['mean'][class_]\n",
    "    sig = train_agg_df[feature]['std'][class_]\n",
    "\n",
    "    Pxy_list = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        obs = data.iloc[i][feature]\n",
    "        p_x_given_y = stats.norm.pdf(obs, loc=mu, scale=sig)\n",
    "        Pxy_list.append(p_x_given_y)\n",
    "\n",
    "    return Pxy_list[obs_idx]\n",
    "\n",
    "point_est = p_x_given_class(train_df,'age',0)\n",
    "point_est\n",
    "\n",
    "0.035036938123834606\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Predict class for a given observation;  \n",
    "- For each possible class, take the product of the point estimates for the observation and multiply them by the probability of the given class. Take the class with the greatest probability\n",
    "\n",
    "```Python\n",
    "def predict_class(obs_row):\n",
    "    c_probs = []\n",
    "    for c in range(2):\n",
    "        # Initialize probability to relative probability of class\n",
    "        p = len(y_train[y_train == c])/len(y_train) \n",
    "        for feature in X.columns:\n",
    "            p *= p_x_given_class(obs_row, feature, c)\n",
    "        c_probs.append(p)\n",
    "    return np.argmax(c_probs)\n",
    "\n",
    "predict_class(X_train.iloc[0])\n",
    "\n",
    "0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 5: Evaluate the model using some chosen evaluation metric.  \n",
    "\n",
    " ```Python\n",
    "y_hat_train = [predict_class(X_train.iloc[idx]) for idx in range(len(X_train))]\n",
    "y_hat_test = [predict_class(X_test.iloc[idx]) for idx in range(len(X_test))]\n",
    "\n",
    "residuals_train = y_hat_train == y_train\n",
    "acc_train = residuals_train.sum()/len(residuals_train)\n",
    "\n",
    "residuals_test = y_hat_test == y_test\n",
    "acc_test = residuals_test.sum()/len(residuals_test)\n",
    "print('Training Accuracy: {}\\tTesting Accuracy: {}'.format(acc_train, acc_test))\n",
    "\n",
    "Training Accuracy: 0.8502202643171806\tTesting Accuracy: 0.8289473684210527\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Classification with Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Preprocess data\n",
    "Load, clean and train-test split just like above.\n",
    "\n",
    "```Python\n",
    "# Load data\n",
    "df = pd.read_csv('SMSSpamCollection', sep='\\t', names=['label', 'text'])\n",
    "df.head()\n",
    "```\n",
    "<img src='images/word_table.png' height = 176>\n",
    "\n",
    "```Python\n",
    "# deal with class imbalance\n",
    "p_classes = dict(df.label.value_counts(normalize=True))\n",
    "p_classes\n",
    "\n",
    "{'ham': 0.8659368269921034, 'spam': 0.13406317300789664}\n",
    "\n",
    "minority = df[df['label']=='spam']\n",
    "majority_undersampled = df[df['label']=='ham'].sample(n=len(minority))\n",
    "\n",
    "undersampled_df = pd.concat([minority,majority_undersampled])\n",
    "\n",
    "undersampled_df['label'].value_counts()\n",
    "\n",
    "ham     747\n",
    "spam    747\n",
    "Name: label, dtype: int64\n",
    "\n",
    "# train-test split sampling\n",
    "X = undersampled_df['text']\n",
    "y = undersampled_df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=17)\n",
    "train_df = pd.concat([X_train,y_train],axis=1)\n",
    "test_df = pd.concat([X_test,y_test],axis=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Discover word frequency for each target class\n",
    "\n",
    "```Python\n",
    "class_word_freq = {} \n",
    "classes = train_df['label'].unique()\n",
    "\n",
    "for class_ in classes:\n",
    "    temp_df = train_df[train_df.label == class_]\n",
    "    bag = {}\n",
    "\n",
    "    for row in temp_df.index:\n",
    "        doc = temp_df['text'][row]\n",
    "\n",
    "        for word in doc.split():\n",
    "            bag[word] = bag.get(word, 0) + 1\n",
    "            \n",
    "    class_word_freq[class_] = bag\n",
    "\n",
    "class_word_freq\n",
    "\n",
    "{'ham': {'Its': 7,\n",
    "  'ok,': 1,\n",
    "  'if': 26,\n",
    "  'anybody': 1,\n",
    "  'asks': 1,\n",
    "  'abt': 4,\n",
    "  'me,': 3,\n",
    "  'u': 85,\n",
    "  'tel': 1,\n",
    "  'them..:-P': 1,\n",
    "  'Thanx': 2,\n",
    "  '4': 27,\n",
    "  'e': 15,\n",
    "  'brownie': 1,\n",
    "  \"it's\": 7,\n",
    "  'v': 2,\n",
    "  'nice...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Get the total corpus word count\n",
    "\n",
    "```Python\n",
    "vocabulary = set()\n",
    "for text in train_df['text']:\n",
    "    for word in text.split():\n",
    "        vocabulary.add(word)\n",
    "V = len(vocabulary)\n",
    "\n",
    "V\n",
    "5904\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Implement Naive Bayes algorithm\n",
    "\n",
    "```Python\n",
    "# create a bag of words function\n",
    "def bag_it(doc):\n",
    "    bag = {}\n",
    "    for word in doc.split():\n",
    "        bag[word] = bag.get(word, 0) + 1\n",
    "    return bag\n",
    "\n",
    "def classify_doc(doc, class_word_freq, p_classes, V, return_posteriors=False):\n",
    "    bag = bag_it(doc)\n",
    "    classes = []\n",
    "    posteriors = []\n",
    "    for class_ in class_word_freq.keys():\n",
    "        p = np.log(p_classes[class_])\n",
    "        for word in bag.keys():\n",
    "            num = bag[word]+1\n",
    "            denom = class_word_freq[class_].get(word, 0) + V\n",
    "            p += np.log(num/denom)\n",
    "        classes.append(class_)\n",
    "        posteriors.append(p)\n",
    "    if return_posteriors:\n",
    "        print(posteriors)\n",
    "    return classes[np.argmax(posteriors)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Evaluate classifier performance\n",
    "\n",
    "```Python\n",
    "y_hat_train = X_train.map(lambda x: classify_doc(x, class_word_freq, p_classes, V))\n",
    "residuals = y_train == y_hat_train\n",
    "residuals.value_counts(normalize=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8eb6b2671ae257969e1e6d572d1bfd2dc1c23f390a7081d26c146f9b82ef978d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('learn-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

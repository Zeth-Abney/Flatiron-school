{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 26 Topic Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Confusion Matrices \n",
    "- Create a confusion matrix from scratch \n",
    "- Create a confusion matrix using scikit-learn \n",
    "- Visualize confusion matrices \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Create a confusion matrix from scratch**\n",
    "```Python\n",
    "def conf_matrix(y_true, y_pred):\n",
    "    conf_dict = {'TP':0,'TN':0,'FP':0,'FN':0}\n",
    "    y_true = list(y_true)\n",
    "    y_pred = list(y_pred)\n",
    "\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == y_pred[i]: # True \n",
    "            if y_pred[i] == 1:\n",
    "                conf_dict['TP'] += 1 #Positive\n",
    "            else:\n",
    "                conf_dict['TN'] += 1 #Negative\n",
    "\n",
    "        else: #False\n",
    "            if y_pred[i] == 1: #Positive\n",
    "                conf_dict['FP'] += 1\n",
    "            else: #Negative\n",
    "                conf_dict['FN'] += 1\n",
    "\n",
    "    return conf_dict\n",
    "\n",
    "# Example output: {'TP': 38, 'TN': 26, 'FP': 7, 'FN': 5}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Create a confusion matrix using scikit-learn**\n",
    "\n",
    "```Python\n",
    "# Import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Print confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test,y_predicted)\n",
    "print('Confusion Matrix:\\n', cnf_matrix)\n",
    "\n",
    "#example output\n",
    "Confusion Matrix:\n",
    " [[26  7]\n",
    " [ 5 38]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Visualize confusion matrices**\n",
    "\n",
    "```Python\n",
    "# Import plot_confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "# Visualize your confusion matrix\n",
    "plot_confusion_matrix(model_log,X_test,y_test)\n",
    "plt.show()\n",
    "```\n",
    "<img src='images/output.png' width=350>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Logistic Regression models\n",
    "- Implement evaluation metrics from scratch using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision** is how often a prediction is correct, or in other words what is the liklihood that a given prediction is correct.  \n",
    " \n",
    "$\\Large \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} $  \n",
    "\n",
    "\n",
    "```Python\n",
    "def precision(y, y_hat):\n",
    "    y_y_hat = list(zip(y,y_hat))\n",
    "    true_pos = sum([1 for i in y_y_hat if i[0]==1 and i[1]==1])\n",
    "    false_pos = sum([1 for i in y_y_hat if i[0]==0 and i[1]==1])\n",
    "    return true_pos/float(true_pos + false_pos)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall** is an indication of what percentage of the target classes are captured by the model.\n",
    "\n",
    "$\\Large \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} $  \n",
    "\n",
    "```Python\n",
    "def recall(y, y_hat):\n",
    "    y_y_hat = list(zip(y,y_hat))\n",
    "    true_pos = sum([1 for i in y_y_hat if i[0]==1 and i[1]==1])\n",
    "    false_neg = sum([1 for i in y_y_hat if i[0] == 1 and i[1] == 0])\n",
    "    return true_pos/float(true_pos + false_neg)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy** is the proportion of total observations that are predicted correctly (both positive and negative)  \n",
    "\n",
    "$\\Large \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} $  \n",
    "\n",
    "```Python\n",
    "def accuracy(y, y_hat):\n",
    "    # Your code here\n",
    "    y_y_hat = list(zip(y,y_hat))\n",
    "    true_pos = sum([1 for i in y_y_hat if i[0]==1 and i[1]==1])\n",
    "    true_neg = sum([1 for i in y_y_hat if i[0] == 0 and i[1] == 0])\n",
    "    return (true_pos + true_neg)/float(len(y_hat))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **F1 score** represents the *harmonic mean of precision and recall*; essentially meaning that for the F1 score to be high, both precision and recall must be high.  \n",
    "\n",
    "$\\Large \\text{F1 score} = 2 * \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}} $  \n",
    "\n",
    "```Python\n",
    "def f1_score(y, y_hat):\n",
    "    # Your code here\n",
    "    Precision = precision(y,y_hat)\n",
    "    Recall = recall(y,y_hat)\n",
    "    numer = Precision * Recall\n",
    "    denom = Precision + Recall\n",
    "    return 2*(numer/denom)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curves and AUC\n",
    "- Create a visualization of ROC curves and use it to assess a model \n",
    "- Evaluate classification models using the evaluation metrics appropriate for a specific problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Receiver Operator Characteristic curve (ROC curve) illustrates the true positive rate against the false positive rate of our classifier.  \n",
    "\n",
    "$$ \\text{TPR} = \\frac{\\text{TP}}{\\text{TP}+\\text{FN}} $$     \n",
    "\n",
    "$$ \\text{FPR} = \\frac{\\text{FP}}{\\text{FP}+\\text{TN}}$$\n",
    "\n",
    "<center><img src='images/decision_boundary.png' width=400></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "# First calculate true positive rate(TPR) and false positive rate(FPR)\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "y_score = logreg.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "\n",
    "# Calculate are under curve and plot roc curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Seaborn's beautiful styling\n",
    "sns.set_style('darkgrid', {'axes.facecolor': '0.9'})\n",
    "\n",
    "print('AUC: {}'.format(auc(fpr, tpr)))\n",
    "plt.figure(figsize=(10, 8))\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "\n",
    "<img src='images/seaborn_roc.png' width=550>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Model Comparisons\n",
    "- Compare the different inputs with logistic regression models and determine the optimal model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple roc curves (and auc) should be calculated by adjusting various hyperparameters of the model, such as:   \n",
    "- the regularization term 'C'\n",
    "- the  penalty term (e.g. L1,L2,etc.)\n",
    "- the class weights  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/roc_comparison.png' width=550>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Imbalance Problems\n",
    "- Use sampling techniques to address a class imbalance problem within a dataset \n",
    "- Create a visualization of ROC curves and use it to assess a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class imbalance occurs when an overwhelming majority of observations belong to a particular class and therefore only a small minory belongs to the other. \n",
    "\n",
    "```Python\n",
    "print('Raw counts: \\n')\n",
    "print(df['Class'].value_counts())\n",
    "print('-----------------------------------')\n",
    "print('Normalized counts: \\n')\n",
    "print(df['Class'].value_counts(normalize=True))\n",
    "\n",
    "Raw counts: \n",
    "\n",
    "0    284315\n",
    "1       492\n",
    "Name: Class, dtype: int64\n",
    "-----------------------------------\n",
    "Normalized counts: \n",
    "\n",
    "0    0.998273\n",
    "1    0.001727\n",
    "Name: Class, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [SMOTE](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html) function from the imblearn library can be used to improve a model's performance on the minority class.  SMOTE stands for **Synthetic Minority Oversampling**\n",
    "\n",
    "```Python\n",
    "# Previous original class distribution\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Fit SMOTE to training data\n",
    "X_train_resampled, y_train_resampled = SMOTE().fit_resample(X_train,y_train)\n",
    "\n",
    "# Preview synthetic sample class distribution\n",
    "print('\\n')\n",
    "print(pd.Series(y_train_resampled).value_counts()) \n",
    "\n",
    "0    213233\n",
    "1       372\n",
    "Name: Class, dtype: int64\n",
    "\n",
    "1    213233\n",
    "0    213233\n",
    "Name: Class, dtype: int64\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Thank you for exploring this notebok and viewing my content! Click the links below to explore further!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---> [LIKE AND SUBSCRIBE TO THE YOUTUBE CHANNEL](https://www.youtube.com/channel/UCCrhddH1eLvb0iolx28r_ig)\n",
    "\n",
    "#### ---> [EXPLORE MY GITHUB](https://github.com/Zeth-Abney/Flatiron-school)\n",
    "\n",
    "#### ---> [CONNECT WITH ME ON LINKEDIN](https://www.linkedin.com/in/zeth-abney/)\n",
    "\n",
    "#### ---> [DISCOVER WHAT FLATIRON SCHOOL HAS TO OFFER](https://flatironschool.com/courses/data-science-bootcamp/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8eb6b2671ae257969e1e6d572d1bfd2dc1c23f390a7081d26c146f9b82ef978d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('learn-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

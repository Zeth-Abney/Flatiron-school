{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 31 Topic Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods are modeling strategies that involve utilizing multiple models in concert.  \n",
    "\n",
    "Ensemble models are highly resilient to variance and noise so they tend to make better predictions than any single model.  \n",
    "\n",
    "<img src='images/bias-variance.png' width=450>\n",
    "\n",
    "The disadvantage is they very computationally expensive, between data fitting, hyperparameter tuning, and validation . . . potentially thousands of prototype models have to be computed to arrive at a final ensemble model.  \n",
    "\n",
    "The foundational technique that makes ensembling possible is data *bagging* (bootstrap aggregation). This involves bootstrap sampling data (sampling with replacement) for each model in the ensemble and then aggregating that outputs of each individual model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests are an ensemble of tree-based models (regression or decision trees).  \n",
    "\n",
    "Trees are entirely deterministic (there is no randomness), so in order to have a variety of trees in the ensemble each tree is given a different subset of the data.  \n",
    "\n",
    "Bagging here is accomploshing by -for each tree- randomly bootstrap sampling two-thirds of the data for training, and using the remaining third is referred to as *OOB* (out of bag data) and is used as test data to calculate the *OOB error*.  \n",
    "\n",
    "In order to create further variability in the ensemble, after bagging the data each tree is then randomly sampled from the *sub-space*, meaning a given number of features are then randomly sampled (others dropped) from each tree.  \n",
    "\n",
    "This rigorous bagging and subspace sampling creates a model that is extremely resilient to over fitting, but this comes at the cost of computational complexity and a large amount of memory needed to instantiate and maintain so many trees. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Boosting and Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is a group of ensemble algorithms that are similarly effective to random forest but also have some notable differences in their logic. \n",
    "\n",
    "Boosting algorithms build models iteratively (i.e. exactly how one model is built is affected but how the previous one was built)  \n",
    "\n",
    "Boosters utilize \"weak learners\", meaning any single model is not very good in general but very good with one particular aspect of the dataset. \n",
    "\n",
    "Boosters aggregate differently. Random forests use a voting system to decide on the final output. Boosters use a weighting system that assigns importance to each tree in the ensemble, so some models have \"more pull\" over the final output than others.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Adaboost was the first boosting algorithm invented. When it validates a single model, the data points that were misclassified get weighted heavier in the next model.  \n",
    "\n",
    "<img src='images/adaboost.png' width=550>\n",
    "\n",
    "\"**Key Takeaway**: *Adaboost creates new classifiers by continually influencing the distribution of the data sampled to train each successive learner.*\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Gadient-Boosting  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Gradient boost is similar in nature to Adaboost but slightly more sophisticated. After validating a model (which is a weak learner) instead of simply weighting missclassified points heavier, gradient boosting calculates the residuals of the missclasification which is then passed into a loss function to calculate overall loss, which can then be used to calculate the gradient for the loss. The overall loss and gradient are then used as predictors when computing the following model. \n",
    "\n",
    "<img src='images/gradient-boost.png' width=800>\n",
    "\n",
    "XGboost is the premiere gradient boosting algorithm currently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('learn-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8eb6b2671ae257969e1e6d572d1bfd2dc1c23f390a7081d26c146f9b82ef978d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

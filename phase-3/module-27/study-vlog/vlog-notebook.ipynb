{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 27 Topic Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees are a type of classifier that partitions the sample sapce recursively.  \n",
    "\n",
    "Graph theory defines them as a D.A.G. A ***Directed Acyclic Graph*** where:  \n",
    "- There is one root node with no incoming edges  \n",
    "- interior nodes have only one input and two or more outputs\n",
    "- Leaf/terminal nodes have no output. \n",
    "\n",
    "There are various algorithms available to grow a decision tree including ID3, and CART.\n",
    "- **ID3** trees use some pre-sepcified criterion to partition the sample space such as . . .  \n",
    "  \n",
    "    *entropy*  . . . $H(s) = -\\sum{}(P_i\\log_2{P_i})$  \n",
    "      \n",
    "    *information gain* . . . $IG(A,S) = H(s) - \\sum_{}p(t)H(t)$ \n",
    "\n",
    "- **CART** (Classification And Regression Trees) uses *Mean Squared Error* as the partition criterion . . .  \n",
    "\n",
    "    Each step in the tree selects the value for $\\theta$ *theta* that minimizes the related cost function . . .  \n",
    "    \n",
    "    $J(D, \\theta) = \\frac{n_{left}}{n_{total}} MSE_{left} + \\frac{n_{right}}{n_{total}} MSE_{right}$\n",
    "\n",
    "    <img src='images/dt2.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Pruning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various *\"hyperparameters\"* exist which can affect the performance of the model. For example:  \n",
    "- Maximum depth; how many steps down the model partitions. Too much depth can lead to overfitting.\n",
    "- The minimum samples to make a split.\n",
    "- Minimum samples to make a leaf node. \n",
    "\n",
    "<img src='images/tree.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classification, in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3 Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#import data\n",
    "df = pd.read_csv('cool_data.csv')\n",
    "\n",
    "# declare target and predictor features\n",
    "y = df.Target\n",
    "X = df.drop('Target',axis=1)\n",
    "\n",
    "# sample data with train-test split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=10)\n",
    "\n",
    "#instantiate and fit a classifier\n",
    "classifier = DecisionTreeClassifier(random_state=10,criterion='entropy')\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "# evaluate predictive performance with roc curves\n",
    "accuracy = accuracy_score(y_test,y_pred) * 100\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_pred)\n",
    "\n",
    "area_under_curve = auc(false_positive_rate,true_positive_rate)\n",
    "\n",
    "# Prune the tree \n",
    "# Identify the optimal tree depth for given data\n",
    "max_depth = np.linspace(1,32,32,endpoint=True)\n",
    "train_results = []\n",
    "test_results = []\n",
    "for i in max_depth:\n",
    "    dtc = DecisionTreeClassifier(criterion='entropy', max_depth=i, random_state=SEED)\n",
    "    dtc.fit(X_train,y_train)\n",
    "    train_pred = dtc.predict(X_train)\n",
    "    fpr, tpr, thresh = roc_curve(y_train,train_pred)\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "    train_results.append(roc_auc)\n",
    "\n",
    "    test_pred = dtc.predict(X_test)\n",
    "    fpr,tpr,thresh = roc_curve(y_test,test_pred)\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "    test_results.append(roc_auc)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(max_depth, train_results, 'b', label='Train AUC')\n",
    "plt.plot(max_depth, test_results, 'r', label='Test AUC')\n",
    "plt.ylabel('AUC score')\n",
    "plt.xlabel('Tree depth')\n",
    "plt.legend()\n",
    "plt.show() \n",
    "```\n",
    "<img src='images/depth.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CART Tree classification\n",
    "```Python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "#import data\n",
    "df = pd.read_csv('cool_data.csv')\n",
    "\n",
    "# declare target and predictor features\n",
    "y = df.Target\n",
    "X = df.drop('Target',axis=1)\n",
    "\n",
    "# sample data with train-test split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=10)\n",
    "\n",
    "# Instantiate and fit a regression tree model to training data \n",
    "regressor = DecisionTreeRegressor()\n",
    "regressor.fit(X_train,y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Evaluate these predictions\n",
    "print('Mean Absolute Error:', mean_absolute_error(y_test,y_pred))  \n",
    "print('Mean Squared Error:', mean_squared_error(y_test,y_pred)) \n",
    "print('Root Mean Squared Error:', np.sqrt(mean_squared_error(y_test,y_pred)))\n",
    "\n",
    "# Prune the tree\n",
    "\n",
    "min_samples_split = np.arange(2,11)\n",
    "mse_results = []\n",
    "r2_results = []\n",
    "\n",
    "for m in min_samples_split:\n",
    "    regr = DecisionTreeRegressor(random_state=45,min_samples_split=m)\n",
    "    regr.fit(x_test,y_test)\n",
    "    y_pred = regr.predict(x_test)\n",
    "    score = performance(y_test,y_pred)\n",
    "    r2_results.append(score[0])\n",
    "    mse_results.append(score[1])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(min_samples_split, r2_results, 'b', label='R2')\n",
    "plt.xlabel('minimum sample split')\n",
    "plt.ylabel('R-squared')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(min_samples_split, mse_results, 'r', label='RMSE')\n",
    "plt.xlabel('Minimum sample split')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "<img src='images/min_split.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8eb6b2671ae257969e1e6d572d1bfd2dc1c23f390a7081d26c146f9b82ef978d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('learn-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

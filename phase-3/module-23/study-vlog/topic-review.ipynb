{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 23 topic review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f'(x) = \\dfrac{\\Delta y}{\\Delta x} =  \\dfrac{f(x + \\Delta x) - f(x)}{\\Delta x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A derivative is the instantaneous rate of change of a function (i.e. the slope of a curve)\n",
    "- if the rate of change is calculated in terms of time, as the time-frame approaches zero the derivative approaches a *limit*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivatives of linear functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivatives of linear functions are simply the distance between two vector points \n",
    "$$\\large f'(x) = \\dfrac{\\Delta y}{\\Delta x} = \\dfrac{y_{2}-y_{1}}{x_{2}-x_{1}}$$  \n",
    "<img src=\"images/linear_derivative.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules for calculating derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivatives of non-linear functions are a little trickier because the difference between any two vector points along the curve is in fact shorter than the distance actually covered by the curve.  \n",
    "\n",
    "To deal with this issue we calculate the derivative's *limit* which is the value the derivative approaches as the linear distance between the two vector points approaches zero. \n",
    "\n",
    "So the derivative formula for non-linear functions is then defined as:  \n",
    "$$ f'(x) = \\displaystyle {\\lim_{ \\Delta x \\to 0}} \\frac{f(x + \\Delta x) - f(x)}{\\Delta x} $$  \n",
    "\n",
    "<img src = \"images/non-linear-derivatives.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules for calculating derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The power rule**\n",
    "-  if a variable,  ùë• , is raised to a exponent  ùëü , then the derivative of that function is the exponent  ùëü  multiplied by the variable, with the variable raised to the original exponent minus one.\n",
    "- given that: $ f(x) = x^r $\n",
    "- therefore: $ f'(x) = r*x^{r-1} $\n",
    "- example . . .\n",
    "    - given the function: $f(x) = x^2 $\n",
    "    - therefore: $f'(x) = 2*x^{2-1} = 2*x^1 = 2*x $  \n",
    "\n",
    "**The constant factor rule**\n",
    "- If the variable is multiplied by a constant, then take the derivative of the variable first and then multiply the constant by the derivative\n",
    "- the general case is defined as: $\\frac{\\Delta f}{\\Delta x}(a*f(x)) = a * \\frac{\\Delta f}{\\Delta x}*f(x) $\n",
    "- example . . . \n",
    "    - given the function: $f(x) = 2x^2 $\n",
    "    - therefore: $f'(x) = 2*\\frac{\\Delta f}{\\Delta x} x^{2} = 2*2*x^{2-1} = 4x^1 = 4x $\n",
    "\n",
    "**The addition rule**\n",
    "- To take a derivative of a function that has multiple terms, simply take the derivative of each of the terms individually.\n",
    "- example:\n",
    "    - given the function: $ f(x) = 4x^3 - x^2 + 3x $\n",
    "    - therefore: $ f'(x) = 12x^2 - 2x + 3  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A visualization of derivatives calculated at various values for x on a non-linear function\n",
    "<img src = \"images/tangent-lines.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minima and maxima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Minima or maxima of a non-linear function can be simply understood as the peak of a curve.  \n",
    "\n",
    "Given a non-linear function $f(x)$ the value of $x$ at the functions minima (or maxima for the inverse) will be the same value for x where $f'(x) = 0$\n",
    "\n",
    "An example:  \n",
    "    - given the function: $f(x) = 2x^2-8x$  \n",
    "    - the derivative is: $f'(x) = 4 x - 8 $  \n",
    "    - so the optimum x-value is where $f'(x) = 4x - 8 = 0 $  \n",
    "    - which simplifies to $ x= 2$  \n",
    "\n",
    "visualized . . .   \n",
    "<img src = \"images/minima.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "Gradient descent is an optimization algorithm used to find the values of a function's coefficients that minimize the related cost function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: begin with a regression line with a best-guess for $m$ and $b$  \n",
    "2: calculate the residual sum of squares $(RSS)$  \n",
    "3: adjust $m$ (x-axis), and/or $b$ (y-axis)  \n",
    "4: re-calculate $RSS$  \n",
    "5: repeat process . . .  \n",
    "6: select the values for $m$ and $b$ where $RSS$ is the least.  \n",
    "    - The change in outpute based in the change in input(of m or b) is the **cost function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount that the parameter is changed is referred to as the *step size*.  \n",
    "This sign and slope of a curve can help determine what the appropriate step size is.  \n",
    "- if the slope approaches $0$ on the y-axis (i.e. is negative), the next step should move towards $\\infty$ on the x-axis (i.e. away from zero). (and vice-versa)\n",
    "- As the slope moves away from $0$(i.e. absolute value increases), the larger the step size should be.\n",
    "\n",
    "The general procedure to find the ideal $m$ is as follows: \n",
    "1.  Randomly choose a value of $m$, and \n",
    "2.  Select some step size $(\\eta)$, then\n",
    "3.  Update $m$ with the formula $ m = (\\eta) * slope_{m = i} + m_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def gradient_descent(x_values, y_values, steps, current_b, learning_rate, m):\n",
    "    cost_curve = []\n",
    "    for i in range(steps):\n",
    "        cost_slope = slope_at(x_values,y_values,m,current_b)\n",
    "        current_rss = residual_sum_squares(x_values,y_values,m,current_b)\n",
    "        cost_curve.append({'b':current_b,'rss':current_rss,'slope':cost_slope})\n",
    "        current_b = updated_b(current_b,learning_rate,cost_slope)\n",
    "    return cost_curve\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent in three-dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For non-linear multi-variable functions (e.g. $f(x, y) = y*x^2 $) the related cost-function will be three dimensional. \n",
    "\n",
    "In this case the formula for residuals sum of squares is defined as:  \n",
    "$J(m, b) = \\sum_{i=1}^{n}(y_i - (mx_i + b))^2$  \n",
    "\n",
    "This means adjusting $m$ *and* $b$ in tandem, which requires taking the *partial derivative* of either parameter while treating the other as a constant.  \n",
    "<img src = \"images/multi-variable-function.png\" width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the gradient of the cost function for a multi-variable function we take the partial derivative of each parameter with respect to the other as a constant. . .  \n",
    "$$ \\nabla J(m, b) = \\frac{\\delta J}{\\delta m}, \\frac{\\delta J}{\\delta b}$$  \n",
    "\n",
    "For example:\n",
    "- given the formula $f(x, y) = y*x^2 $\n",
    "- Each partial derivative is solved for as . . .  \n",
    "$$ \\frac{dJ}{dm}J(m,b) = -2*\\sum_{i=1}^n x(y_i - \\hat{y}_i)  = -2*\\sum_{i=1}^n x_i*\\epsilon_i$$\n",
    "$$ \\frac{dJ}{db}J(m,b) = -2*\\sum_{i=1}^n(y_i - \\hat{y}_i) = -2*\\sum_{i=1}^n \\epsilon_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practically speaking, the code would be something along the lines of . . .  \n",
    "\n",
    "`current_m` =  `old_m` $ - \\frac{dJ}{dm}J(m,b)$\n",
    "\n",
    "`current_b` =  `old_b` $ - \\frac{dJ}{db}J(m,b) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "np.random.seed(11)\n",
    "\n",
    "x1 = np.random.rand(100,1).reshape(100)\n",
    "x2 = np.random.rand(100,1).reshape(100)\n",
    "y_randterm = np.random.normal(0,0.2,100)\n",
    "y = 2+ 3* x1+ -4*x2 + y_randterm\n",
    "\n",
    "data = np.array([y, x1, x2])\n",
    "data = np.transpose(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient_multi(b_current, m_current ,points):\n",
    "    b_gradient = 0\n",
    "    m_gradient = np.zeros(len(m_current))\n",
    "    learning_rate = .1 # arbitrarialy selected\n",
    "    N = float(len(points)) # weights the gradient proprtional to the size of the data set\n",
    "    for i in range(0, len(points)):\n",
    "        y = points[i][0]\n",
    "        x = points[i][1:(len(m_current)+1)] \n",
    "        b_gradient += -(1/N)  * (y -  (sum(m_current * x) + b_current))\n",
    "        m_gradient += -(1/N) * x * (y -  (sum(m_current * x) + b_current))\n",
    "    new_b = b_current - (learning_rate * b_gradient)\n",
    "    new_m = m_current - (learning_rate * m_gradient)\n",
    "    return (new_b, new_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 0\n",
    "m = [0,0]\n",
    "iterations = []\n",
    "for i in range(500):\n",
    "    iteration = step_gradient_multi(b, m, data)\n",
    "    b= iteration[0]\n",
    "    m = []\n",
    "    for j in range(len(iteration)):\n",
    "        m.append(iteration[1][j])\n",
    "    iterations.append(iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.9431512015830243, array([ 2.99522584, -3.90814716])),\n",
       " (1.9434733566282547, array([ 2.99539515, -3.90888235])),\n",
       " (1.9437935889183264, array([ 2.99556229, -3.90961205])),\n",
       " (1.9441119102745354, array([ 2.99572729, -3.9103363 ])),\n",
       " (1.944428332442866, array([ 2.99589015, -3.91105514]))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterations[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Module 22 topic review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Overview\n",
    "- Data types (scalar, vector, matrix, tensor)\n",
    "- systems of equations (substitution, elimination)\n",
    "- matrix algebra (dot product, identity matrix, inverse)\n",
    "- linear regression (OLS, e, beta vector, (N)RMSE, Big O notation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systems of equations\n",
    "A \"linear system\" is two or more linear equations that share the same set of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### example problem:\n",
    "\n",
    "Jim has more money than Bob. If Jim gave Bob 20 dollars, they would have the same amount. If Bob gave Jim 22 dollars, however, Jim would then have twice as much as Bob. \n",
    "\n",
    "How much does each one actually have?\n",
    "\n",
    "> Let x be the amount of money that Jim has and y be the amount that Bob has "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```John has more money than bob```  \n",
    "> therefore x > y  \n",
    "\n",
    "```If Jim gave Bob 20 dollars, they would have the same amount.```  \n",
    "> Eq.A . . . x - 20 = y + 20  \n",
    "> Eq.B . . . x + 22 = 2(y - 22)  \n",
    "\n",
    "```Using elimination isolate x in Eq.A```  \n",
    "> find that x = y + 40 (let this be Eq.C) \n",
    "\n",
    "```Substitute x = y + 40 in Eq.B```\n",
    "> (y + 40) + 22 = 2(y - 22)  \n",
    "> (y / 2) + 20 + 11 = y - 22  \n",
    "> ( (y / 2) + 53  = y ) * 2  \n",
    ">  ***[ y = 106 ]*** i.e. Bob has $106       \n",
    "\n",
    "```Substitute y = 106 in Eq.C```  \n",
    "> x = 106 + 40  \n",
    "> ***[ x = 146 ]*** i.e. Jim has $146  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Algebra with Matrices\n",
    "<img src=\"images/vector_types.png\" width = \"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Hadamard product`: element-wise product of each indiviudual scalar\n",
    "- both arrays must have like dimensions\n",
    "- defined as . . . $C = A \\circ B$  \n",
    "- availbe in base Python simply as . . . *A * B*\n",
    "\n",
    "`Cross product`: results in a vector perpendicular to each vector being multiplied\n",
    "- not viable beyond 3 dimensions\n",
    "- defined as . . . $a \\times b = \\mid a \\mid  \\mid b \\mid \\sin(\\theta) n $\n",
    "- available in numpy with *np.cross(x,y)*\n",
    "\n",
    "`Dot product`: most commonly used, at least for solving systems of equations (matrix algebra)\n",
    "- **array A must have the same number of columns as array B has rows!**\n",
    "- defined as . . . $C_{i, j}= \\sum_k A_{i, k}B_{k, j}$\n",
    "- available in numpy with . . . *C = A.dot(B)*  \n",
    "\n",
    "`Transposition`: Convert a row -or set of row- vectors into column vector(s)\n",
    "- rotate the matrix right 90° clockwise\n",
    "- reverse the order of elements in each row (e.g. [a b c] becomes [c b a])\n",
    "- available in numpy with *array.T* or *np.transpose(array)*  \n",
    "\n",
    "> <img src=\"images/transpose.png\" width=300>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special matrices:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`The identity matrix`: essentially the matrix version of 1 (i.e. *1 * n = n*)  \n",
    "- The identity matrix is a square matrix (m = n) in which all scalars along the major diagonal (top left to bottom right) are ones and all other elements are zeros\n",
    "- The identity matrix of $\\left[ {\\begin{array}{cc}1 & 2  \\\\3 & 4  \\\\\\end{array} } \\right]$ is $\\left[ {\\begin{array}{cc}1 & 0  \\\\0 & 1  \\\\\\end{array} } \\right]$\n",
    "- available in numpy with *np.identity(array)*\n",
    "- the dot product of any matrix and its corresponding identity matrix will be the matrix itself\n",
    "    - $ A \\cdot I = I \\cdot A = A $\n",
    "\n",
    "`The inverse matrix`: the inverse of a square matrix is $A^{-1}$\n",
    "- such that . . . $I = A \\cdot A^{-1}$\n",
    "- where $I$ is the identity matrix\n",
    "- achieves the same result that division *would* were it possible\n",
    "    > Imagine you want to share 10 apples with 2 people.  \n",
    "    > You can divide 10 by 2, or you can take the reciprocal of 2 (which is 0.5)  \n",
    "    > So the answer is: **10 × 0.5 = 5**  - which means they get 5 apples each.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving linear equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`For a single linear equation:`  \n",
    "- $A$ is a matrix of coefficients, $X$ is an unknown variable and $B$ is the output (dot product)  \n",
    "    > $A \\cdot X = B$  \n",
    "- instead of dividing, multiply both sides by the inverse  \n",
    "    > $ A^{-1} \\cdot A \\cdot X = A^{-1} \\cdot B$  \n",
    "- substitute A<sup>-1</sup> for $I$  \n",
    "    > $I \\cdot X = A^{-1} \\cdot B$  \n",
    "- Now remove $I$ because a matrix multiplied by the identity matrix doesn't change anything.  \n",
    "    > $X = A^{-1} \\cdot B$  \n",
    "\n",
    "`For a system of linear equations`:  \n",
    "- Given equation A: $2a + b = 35$  \n",
    "- and equation B: $3a + 4b = 65$  \n",
    "- The matrix notation would be written as:  \n",
    "<img src=\"images/ab.png\" width=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example problem with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A coffee shop is having a sale on coffee and tea. \n",
    "- On day 1, *29 bags of coffee* and *41 bags of tea* were sold, for a total of *$490 dollars*.\n",
    "    > $29c + 41t = 490$\n",
    "- On day 2, they sold *23 bags of coffee* and *41 bags of tea*, for which customers paid a total of *$448 dollars.*  \n",
    "    > $23c + 41t = 448$\n",
    "  \n",
    "***How much does each bag cost?***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# define matrix A to containt coefficients\n",
    "A = np.matrix([[29,41],[23,41]])\n",
    "# define matrix B to contain variables\n",
    "B = np.matrix([490,448])\n",
    "\n",
    "# find inverse of matrix A\n",
    "A_inv = np.linalg.inv(A)\n",
    "\n",
    "# transpose matrix B for the sake of the dot product rule\n",
    "B = B.T\n",
    "\n",
    "# calculate the dot product of A inverse and B transposed\n",
    "X = A_inv.dot(B)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can also be coded much more quickly with:\n",
    "x = np.linalg.solve(A,B)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Fitting Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression analysis is used to fit a mathematical model to a set of observed data points. The resulting regression line represents the models prediction's for future data point observations.   \n",
    "<img src = 'images/fit_model.png' width = 300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A regression line for a simple linear equation is calculates as $ y = mx + c $ where $m$ is a constant scalar vlaue, $x$ is a random scalar variable and $c$ is the y-intercept where $mx = 0$ as show above.  \n",
    "However this same formula when applied to matrix algebra it is defined as $ y = \\boldsymbol{X} b + e $ where $X$ is an input vector $b$ is a random variable and $e$ is the *error term*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For use cases involving only vectors the numpy polyfit function can be used from the polynomial library as show below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "x = np.random.randint(5, size=20)\n",
    "y = np.random.randint(5, size=20)\n",
    "\n",
    "# Fit with polyfit function to get c(intercept) and m(slope)\n",
    "# the degree parameter = 1 to models this as a straight line\n",
    "c, m = polyfit(x, y, 1)\n",
    "\n",
    "# Plot the data points and line calculated from ployfit\n",
    "plt.plot(x, y, 'o') # plot observed data points\n",
    "plt.plot(x, c + (m * x), '-') # plot regression line\n",
    "plt.xticks(x)\n",
    "\n",
    "plt.show()\n",
    "print(c, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ordinary least squares*: commonly used technique to find and minimize $e$\n",
    "\n",
    "Using linear regression is just trying to solve $Xb = y$. If any of the observed points deviate from the model ($e$), you can't find a direct solution.  \n",
    "To find a solution, you can multiply both sides by the transpose of $X$ (i.e. $X^T$). The $X^T$ times $X$ will always allow us to solve for unknown variables.\n",
    "\n",
    "Knowing that $ y = \\boldsymbol{X} b + e $ we can perform matrix algebra to solve for $b$ . . . $ b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\n",
    "\n",
    "With least squares regression, in order to solve for the expected value of weights, $\\hat{X}$, you need to solve the above equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation\n",
    "The goal is to choose the vector  𝑥  for unknown variables to make  𝑒  as small as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A column of ones must be included at the first column index of $X$, this refers to the y-intercept. If this is not included the function constrained to the origin $(0,0)$ severely limiting its predictive ability.  \n",
    "\n",
    "Also, remember that $ C_{(m, k)} = A_{(m, n)} \\cdot B_{(n, k)}$ therefore $A$ must have as many rows as $B$ has columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ordinary least squares method to find regression line and minimize error term\n",
    "import csv \n",
    "data = []\n",
    "\n",
    "with open('../linear-algebra-regression/windsor_housing.csv') as f:\n",
    "    raw = csv.reader(f)\n",
    "    next(raw) # ignore headers\n",
    "\n",
    "    for row in raw:\n",
    "        ones = [1.0] # add column vector of ones\n",
    "        for r in row:\n",
    "            ones.append(float(r))\n",
    "        data.append(ones)\n",
    "\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly sample 80% of the data for training the model, and 20% of the data for testing the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make array of indices\n",
    "indices = np.arange(data.shape[0])\n",
    "\n",
    "# Randomly choose 80% subset of indices without replacement for training\n",
    "train_idx = np.random.choice(indices,size=(round(546*.8)),replace=False)\n",
    "\n",
    "# Choose remaining 20% of indices for testing\n",
    "test_idx = indices[~np.isin(indices,train_idx)]\n",
    "\n",
    "# Subset data\n",
    "train, test = data[train_idx,:],data[test_idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of datasets\n",
    "print ('Raw data Shape: ', data.shape)\n",
    "print ('Train/Test Split:', train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x and y for test and training sets\n",
    "x_train = train[:,:-1]\n",
    "y_train = train [:,-1]\n",
    "\n",
    "x_test = test[:,:-1]\n",
    "y_test = test[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('x_train, y_train, x_test, y_test:', x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the Beta Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that $X$ and $y$ for our data are defined within our code (i.e. as Python objects) we can calculate $\\beta$ by adapting the previous fromula for our code . . . \n",
    " \n",
    "$\\beta = (x_\\text{train}^T. x_\\text{train})^{-1} . x_\\text{train}^T . y_\\text{train}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = np.transpose(x_train) # tranpose x_train\n",
    "XtX = np.dot(Xt,x_train) # dot product of x_train-transposed and x_train\n",
    "XtX_inv = np.linalg.inv(XtX) # get inverze of XtX\n",
    "\n",
    "Xty = np.dot(Xt,y_train) # dot product of x_train and y-train\n",
    "\n",
    "beta = XtX_inv.dot(Xty) # dot product of XtX_inv and Xty\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have defined a set of coefficients the describe the linear mapping between $X$ nad $y$. We'll now use the test data with $b$ to make new predictions for $y$ (i.e. $\\hat{y}$) using the following formula . . .  \n",
    "\n",
    "$\\hat{y} = x\\beta = \\beta_0 + \\beta_1 x_1 +  \\beta_2 x_2 + \\ldots + \\beta_m x_m $ \n",
    "\n",
    "To do this in Python we take the dot product of each row in $x_\\text{test}$ and its corresponding row in $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "\n",
    "for row in x_test:\n",
    "    pred = row.dot(beta)\n",
    "    y_pred.append(pred)\n",
    "    \n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have defined $\\hat{y}$ as a Python object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate the accuracy of the model we begin by visualizing $\\hat{y}$ against $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 10\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.plot(y_pred, linestyle='-', marker='o', label='predictions')\n",
    "plt.plot(y_test, linestyle='-', marker='o', label='actual values')\n",
    "plt.title('Actual vs. predicted values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above visualization is helpful, but in order to get a deeper understanding of the model's predictive ability it is important to understand how the predictions deviate from the observations *individually*.  \n",
    "To do this we calculate the **root mean squared error** which is defined as . . .  \n",
    "$RMSE = \\sqrt{\\sum^N_{i=1}\\dfrac{ (\\text{Predicted}_i-\\text{Actual}_i)^2}{N}}$  \n",
    "\n",
    "This is easily performed in Python as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = []\n",
    "\n",
    "for pred,actual in zip(y_pred,y_test): # find the squared error for each prediction/observation pair\n",
    "    sq_err = (pred-actual)**2 \n",
    "    err.append(sq_err)\n",
    "\n",
    "mean_sq_err = np.array(err).mean() # set of squared errors by N\n",
    "root_mse = np.sqrt(mean_sq_err) # square root of mean squared error\n",
    "\n",
    "print(root_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above returned value is in terms of the dependent variable (in this case house price). For the purposes of multiple regression we can deal with this by *normalizing* the RMSE (i.e. NRMSE).  \n",
    "  \n",
    "Normalized RMSE is defined as . . .  $NRMSE = \\dfrac{RMSE}{max_i y_i - min_i y_i} $\n",
    "\n",
    "This can be easily performed in Python with the following . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrmse = root_mse/(y_train.max() - y_train.min())\n",
    "nrmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned RMSE is 0.08 which is well within the generally accepted range of 0.2-0.5. This is a strong model and a good candidate to guide decision making through qualitative analyis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS is an elegent and intuitive formula but it has its limits. Because it requires both transposition and inversion and various steps in the calculation, it becomes extremely computational expensive with large data sets. *Big data* is generaly described as m >1,000 (more than 1 thousand rows) and n > 1,000,000 (more than 1 million rows).\n",
    "\n",
    "OLS is great for simple linear regresion or even small scale linear regression but with big data it is becomes so computationally expensive that the run-time for the algorithm becomes impractical. \n",
    "\n",
    "The run-time for a particular algorithm is defined in terms of **big O notation** (e.g. $O(\\log{n})$, $O(n)$, $O(n^2)$, $O(n^3)$)  \n",
    "<img src='images/big_o.png' width = 350>\n",
    "\n",
    "OLS linear regression is computed as $(X^T X)^{-1} X^T y$\n",
    "\n",
    "If $X$ is an (n x k) matrix:\n",
    "\n",
    "- $(X^T X)$ takes $O(n*k^2)$ time and produces a (k x k) matrix  \n",
    "- The matrix inversion of a (k x k) matrix takes $O(k^3)$ time  \n",
    "- $(X^T Y)$ takes $O(n*k^2)$ time and produces a (k x k) matrix  \n",
    "- The final matrix multiplication of two (k x k) matrices takes $O(k^3)$ time  \n",
    "\n",
    "***So the Big O running time for OLS is $O(k^{2 * (n+k)})$ - which is pretty expensive***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear algebra, and especially matrix algebra is bizarrely fascinating.  \n",
    "\n",
    "OLS is a powerful tool when used in the right context but there are not very many real-world contexts for it in the modern world of big data.  \n",
    "\n",
    "Understanding the algebra behind OLS can help understand more complex algorithms more deeply and expand your ability to utilize and manipualte them. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "interpreter": {
   "hash": "8eb6b2671ae257969e1e6d572d1bfd2dc1c23f390a7081d26c146f9b82ef978d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('learn-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

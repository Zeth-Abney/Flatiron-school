{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 38 Topic Review: Big Data with PySpark\n",
    "<img src=\"images/social_media.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What *is* big data?  \n",
    "There is no clear definition or consensus on exactly how much data is  considered ***big*** data. There are some rules-of-thumb that point in the right direction however. Some things to consider when determing if you are dealing with \"big data\" or just a large traditional data set are:  \n",
    "- Anything smaller than a terabyte is probably not big data\n",
    "- Big data usually needs to be stored in a distributed data base (i.e. all the data can not fit on single machine)\n",
    "- Big data almost always needs to be computed on a distributed network (i.e. computation is to expensive to take place on a single machine) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 V's of Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Volume:\n",
    "Refers to how much data is generated.  \n",
    "<img src=\"images/rank_users.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Velocity:\n",
    "Refers to how quickly data is generated.  \n",
    "<img src=\"images/internet_minute.jpg\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variety:\n",
    "refers to the range of data types and formats generated.  \n",
    "<img src=\"images/unstructured_data.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Handle Big Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Processing  \n",
    "The two most common ways of organizing computers into a distributed system are the client-server system and peer-to-peer system.\n",
    "\n",
    "The client-server architecture has nodes that make requests to a central server. The server will then decide to accept or reject these requests and send additional methods out to the outer nodes.\n",
    "\n",
    "Peer-to-peer systems allow nodes to communicate with one another directly without requiring approval from a server.\n",
    "\n",
    "<img src='images/types_of_network.png' width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Processing\n",
    "When using a well-developed distributed system, multiple processors can accomplish tasks at a fraction of the time it would take for a single processor to accomplish.  \n",
    "<img src='images/parallel.png' width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the MapReduce Paradigm\n",
    "Even with parallel and distributed processing there can be limitations to how much data can be processed how quickly, there are some techniques available to in sense *massage* the data before actual computation takes place.  \n",
    "One of the techniques used for this purpose is the MapReduce paradigm.  \n",
    "\n",
    "<img src='images/word_count.png' width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PySpark \n",
    "Base Python can not facilitate big data on its own. However, using (Py)Spark it is possible to write python programs to access big data and perform data exploration and machine learning tasks with it. \n",
    "\n",
    "<img src='images/spark_structure.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are writing Spark code, your code is the \"Driver Program\" pictured here. Your code needs to instantiate a SparkContext if we want to be able to use the Spark Unstructured API.  \n",
    "<img src='images/cluster-overview.png' width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "- Big Data usually refers to datasets that grow so large that they become awkward to work with using traditional database management systems and analytical approaches\n",
    "- Big data refers to data that is terabytes (TB) to petabytes (PB) in size\n",
    "- MapReduce can be used to split big datasets up in smaller sets to be distributed over several machines to deal with Big Data Analytics\n",
    "- PySpark can be installed directly on your computer using conda or in a Docker container\n",
    "- When you start working with PySpark, you have to create a SparkContext or SparkSession\n",
    "- The creation or RDDs is essential when working with PySpark\n",
    "- Examples of actions and transformations include collect(), count(), filter(), first(), take(), and reduce()\n",
    "- Machine Learning on the scale of big data can be done with Spark using the ml library"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f13a7413cac724ebd80807245d33d4858bde029add4ca0ec0539bfde95655477"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

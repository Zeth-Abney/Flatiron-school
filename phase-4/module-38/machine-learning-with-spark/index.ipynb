{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Machine Learning with Spark - Lab\n","\n","## Introduction\n","\n","Previously you saw how to manipulate data with Spark DataFrames as well as create machine learning models. In this lab, you're going to practice loading data, manipulating it, preparing visualizations, and fitting it in the Spark MLlib framework. Let's get started!\n","\n","### Objectives\n","\n","In this lab you will: \n","\n","- Load and manipulate data using Spark DataFrames \n","- Create a Spark ML pipeline that transforms data and runs over a grid of hyperparameters "]},{"cell_type":"markdown","metadata":{},"source":["## The Data\n","\n","This dataset is from a Taiwanese financial company, and the task is to determine which individuals are going to default on their credit card based off of characteristics such as limit balance, past payment history, age, marriage status, and sex.\n","\n","You'll use the file `credit_card_default.csv`, which comes from the [UCI ML Repository](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients)."]},{"cell_type":"markdown","metadata":{},"source":["## Initial Data Exploration\n","\n","Get started by writing the relevant import statement and creating a local SparkSession called `spark`, then use that SparkSession to read `credit_card_default.csv` into a Spark SQL DataFrame."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# import necessary libraries\n","from pyspark.sql import SparkSession\n","# initialize Spark Session\n","spark = SparkSession.builder.master('local').getOrCreate()\n","\n","# read in csv to a spark dataframe\n","spark_df = spark.read.csv('credit_card_default.csv',header='true',inferSchema='true')"]},{"cell_type":"markdown","metadata":{},"source":["Use `.head()` to display the first 5 records, and print out the schema."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["[Row(ID=2, LIMIT_BAL=120000.0, SEX='Female', EDUCATION='College', MARRIAGE='Single', AGE=26, PAY_0=-1, PAY_2=2, PAY_3=0, PAY_4=0, PAY_5=0, PAY_6=2, BILL_AMT1=2682.0, BILL_AMT2=1725.0, BILL_AMT3=2682.0, BILL_AMT4=3272.0, BILL_AMT5=3455.0, BILL_AMT6=3261.0, PAY_AMT1=0.0, PAY_AMT2=1000.0, PAY_AMT3=1000.0, PAY_AMT4=1000.0, PAY_AMT5=0.0, PAY_AMT6=2000.0, default=1),\n"," Row(ID=3, LIMIT_BAL=90000.0, SEX='Female', EDUCATION='College', MARRIAGE='Single', AGE=34, PAY_0=0, PAY_2=0, PAY_3=0, PAY_4=0, PAY_5=0, PAY_6=0, BILL_AMT1=29239.0, BILL_AMT2=14027.0, BILL_AMT3=13559.0, BILL_AMT4=14331.0, BILL_AMT5=14948.0, BILL_AMT6=15549.0, PAY_AMT1=1518.0, PAY_AMT2=1500.0, PAY_AMT3=1000.0, PAY_AMT4=1000.0, PAY_AMT5=1000.0, PAY_AMT6=5000.0, default=0),\n"," Row(ID=4, LIMIT_BAL=50000.0, SEX='Female', EDUCATION='College', MARRIAGE='Married', AGE=37, PAY_0=0, PAY_2=0, PAY_3=0, PAY_4=0, PAY_5=0, PAY_6=0, BILL_AMT1=46990.0, BILL_AMT2=48233.0, BILL_AMT3=49291.0, BILL_AMT4=28314.0, BILL_AMT5=28959.0, BILL_AMT6=29547.0, PAY_AMT1=2000.0, PAY_AMT2=2019.0, PAY_AMT3=1200.0, PAY_AMT4=1100.0, PAY_AMT5=1069.0, PAY_AMT6=1000.0, default=0),\n"," Row(ID=5, LIMIT_BAL=50000.0, SEX='Male', EDUCATION='College', MARRIAGE='Married', AGE=57, PAY_0=-1, PAY_2=0, PAY_3=-1, PAY_4=0, PAY_5=0, PAY_6=0, BILL_AMT1=8617.0, BILL_AMT2=5670.0, BILL_AMT3=35835.0, BILL_AMT4=20940.0, BILL_AMT5=19146.0, BILL_AMT6=19131.0, PAY_AMT1=2000.0, PAY_AMT2=36681.0, PAY_AMT3=10000.0, PAY_AMT4=9000.0, PAY_AMT5=689.0, PAY_AMT6=679.0, default=0),\n"," Row(ID=6, LIMIT_BAL=50000.0, SEX='Male', EDUCATION='Graduate', MARRIAGE='Single', AGE=37, PAY_0=0, PAY_2=0, PAY_3=0, PAY_4=0, PAY_5=0, PAY_6=0, BILL_AMT1=64400.0, BILL_AMT2=57069.0, BILL_AMT3=57608.0, BILL_AMT4=19394.0, BILL_AMT5=19619.0, BILL_AMT6=20024.0, PAY_AMT1=2500.0, PAY_AMT2=1815.0, PAY_AMT3=657.0, PAY_AMT4=1000.0, PAY_AMT5=1000.0, PAY_AMT6=800.0, default=0)]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Display the first 5 records\n","spark_df.head(5)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- ID: integer (nullable = true)\n"," |-- LIMIT_BAL: double (nullable = true)\n"," |-- SEX: string (nullable = true)\n"," |-- EDUCATION: string (nullable = true)\n"," |-- MARRIAGE: string (nullable = true)\n"," |-- AGE: integer (nullable = true)\n"," |-- PAY_0: integer (nullable = true)\n"," |-- PAY_2: integer (nullable = true)\n"," |-- PAY_3: integer (nullable = true)\n"," |-- PAY_4: integer (nullable = true)\n"," |-- PAY_5: integer (nullable = true)\n"," |-- PAY_6: integer (nullable = true)\n"," |-- BILL_AMT1: double (nullable = true)\n"," |-- BILL_AMT2: double (nullable = true)\n"," |-- BILL_AMT3: double (nullable = true)\n"," |-- BILL_AMT4: double (nullable = true)\n"," |-- BILL_AMT5: double (nullable = true)\n"," |-- BILL_AMT6: double (nullable = true)\n"," |-- PAY_AMT1: double (nullable = true)\n"," |-- PAY_AMT2: double (nullable = true)\n"," |-- PAY_AMT3: double (nullable = true)\n"," |-- PAY_AMT4: double (nullable = true)\n"," |-- PAY_AMT5: double (nullable = true)\n"," |-- PAY_AMT6: double (nullable = true)\n"," |-- default: integer (nullable = true)\n","\n"]}],"source":["# Print out the schema\n","spark_df.printSchema()"]},{"cell_type":"markdown","metadata":{},"source":["It looks like we have three non-numeric features. For each non-numeric (`string`) feature, select and show all distinct categories."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+\n","|   SEX|\n","+------+\n","|Female|\n","|  Male|\n","+------+\n","\n","+-----------+\n","|  EDUCATION|\n","+-----------+\n","|High School|\n","|          0|\n","|          5|\n","|          6|\n","|      Other|\n","|   Graduate|\n","|    College|\n","+-----------+\n","\n","+--------+\n","|MARRIAGE|\n","+--------+\n","|       0|\n","|   Other|\n","| Married|\n","|  Single|\n","+--------+\n","\n"]}],"source":["# Select and show all distinct categories\n","for column, data_type in spark_df.dtypes:\n","    if data_type == 'string':\n","        # Select and show distinct values in that column\n","        spark_df.select(column).distinct().show()"]},{"cell_type":"markdown","metadata":{},"source":["Interesting...it looks like we have some extraneous values in our categories. For example both `EDUCATION` and `MARRIAGE` have a category `0`.\n","\n","Let's create some visualizations of each of these to determine just how many of them there are.\n","\n","Create bar plots of the variables `EDUCATION` and `MARRIAGE` to see how the records are distributed between the categories.\n","\n","<details>\n","    <summary><u>Click to reveal hint</u></summary>\n","    \n","To create a bar plot, you need to group by the category (`.groupBy()`) and then aggregate by the count in that category (`.count()`). That will result in a small DataFrame containing `EDUCATION` and `count` columns.\n","    \n","Then the easiest way to create a bar plot is to call `.toPandas()` to make that small Spark SQL DataFrame into a pandas DataFrame, and call `.plot()` on the pandas DataFrame.\n","\n","</details>"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+-----+\n","|  EDUCATION|count|\n","+-----------+-----+\n","|High School| 4917|\n","|          0|   14|\n","|          5|  280|\n","|          6|   51|\n","|      Other|  123|\n","|   Graduate|10585|\n","|    College|14029|\n","+-----------+-----+\n","\n"]},{"ename":"ImportError","evalue":"Pandas >= 0.23.2 must be installed; however, it was not found.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[1;32mc:\\Users\\zethu\\Desktop\\Flatiron\\phase-4\\module-38\\machine-learning-with-spark\\index.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zethu/Desktop/Flatiron/phase-4/module-38/machine-learning-with-spark/index.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m education_cats\u001b[39m.\u001b[39mshow()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zethu/Desktop/Flatiron/phase-4/module-38/machine-learning-with-spark/index.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Then plot data\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zethu/Desktop/Flatiron/phase-4/module-38/machine-learning-with-spark/index.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m education_cats\u001b[39m.\u001b[39;49mtoPandas()\u001b[39m.\u001b[39mplot(x\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEDUCATION\u001b[39m\u001b[39m\"\u001b[39m, y\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m\"\u001b[39m, kind\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbar\u001b[39m\u001b[39m\"\u001b[39m, rot\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n","File \u001b[1;32mc:\\Users\\zethu\\anaconda3\\envs\\spark-env\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:63\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, DataFrame)\n\u001b[0;32m     62\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m require_minimum_pandas_version\n\u001b[1;32m---> 63\u001b[0m require_minimum_pandas_version()\n\u001b[0;32m     65\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\zethu\\anaconda3\\envs\\spark-env\\lib\\site-packages\\pyspark\\sql\\pandas\\utils.py:32\u001b[0m, in \u001b[0;36mrequire_minimum_pandas_version\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m     have_pandas \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m have_pandas:\n\u001b[1;32m---> 32\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPandas >= \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must be installed; however, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     33\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mit was not found.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m minimum_pandas_version)\n\u001b[0;32m     34\u001b[0m \u001b[39mif\u001b[39;00m LooseVersion(pandas\u001b[39m.\u001b[39m__version__) \u001b[39m<\u001b[39m LooseVersion(minimum_pandas_version):\n\u001b[0;32m     35\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPandas >= \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must be installed; however, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39myour version was \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (minimum_pandas_version, pandas\u001b[39m.\u001b[39m__version__))\n","\u001b[1;31mImportError\u001b[0m: Pandas >= 0.23.2 must be installed; however, it was not found."]}],"source":["# Create bar plot of EDUCATION\n","education_cats = spark_df.groupBy('EDUCATION').count()\n","education_cats.show()\n","# Then plot data\n","education_cats.toPandas().plot(x=\"EDUCATION\", y=\"count\", kind=\"bar\", rot=0);"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+-----+\n","|MARRIAGE|count|\n","+--------+-----+\n","|       0|   54|\n","|   Other|  323|\n","| Married|13658|\n","|  Single|15964|\n","+--------+-----+\n","\n"]},{"ename":"ImportError","evalue":"Pandas >= 0.23.2 must be installed; however, it was not found.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[1;32mc:\\Users\\zethu\\Desktop\\Flatiron\\phase-4\\module-38\\machine-learning-with-spark\\index.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zethu/Desktop/Flatiron/phase-4/module-38/machine-learning-with-spark/index.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m marriage_cats\u001b[39m.\u001b[39mshow()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zethu/Desktop/Flatiron/phase-4/module-38/machine-learning-with-spark/index.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Then plot data\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zethu/Desktop/Flatiron/phase-4/module-38/machine-learning-with-spark/index.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m marriage_cats\u001b[39m.\u001b[39;49mtoPandas()\u001b[39m.\u001b[39mplot(x\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMARRIAGE\u001b[39m\u001b[39m\"\u001b[39m, y\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m\"\u001b[39m, kind\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbar\u001b[39m\u001b[39m\"\u001b[39m, rot\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n","File \u001b[1;32mc:\\Users\\zethu\\anaconda3\\envs\\spark-env\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:63\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, DataFrame)\n\u001b[0;32m     62\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m require_minimum_pandas_version\n\u001b[1;32m---> 63\u001b[0m require_minimum_pandas_version()\n\u001b[0;32m     65\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\zethu\\anaconda3\\envs\\spark-env\\lib\\site-packages\\pyspark\\sql\\pandas\\utils.py:32\u001b[0m, in \u001b[0;36mrequire_minimum_pandas_version\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m     have_pandas \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m have_pandas:\n\u001b[1;32m---> 32\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPandas >= \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must be installed; however, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     33\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mit was not found.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m minimum_pandas_version)\n\u001b[0;32m     34\u001b[0m \u001b[39mif\u001b[39;00m LooseVersion(pandas\u001b[39m.\u001b[39m__version__) \u001b[39m<\u001b[39m LooseVersion(minimum_pandas_version):\n\u001b[0;32m     35\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPandas >= \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must be installed; however, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39myour version was \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (minimum_pandas_version, pandas\u001b[39m.\u001b[39m__version__))\n","\u001b[1;31mImportError\u001b[0m: Pandas >= 0.23.2 must be installed; however, it was not found."]}],"source":["# Create bar plot of MARRIAGE\n","marriage_cats = spark_df.groupby('MARRIAGE').count()\n","marriage_cats.show()\n","# Then plot data\n","marriage_cats.toPandas().plot(x=\"MARRIAGE\", y=\"count\", kind=\"bar\", rot=0);"]},{"cell_type":"markdown","metadata":{},"source":["## Binning\n","\n","It looks like there are barely any records in the `0`, `5`, and `6` categories. Let's go ahead and bin (combine) those with the current `Other` records into a single catch-all `Other` category for both `EDUCATION` and `MARRIAGE`.\n","\n","The approach we'll use is similar to the `CASE WHEN` technique in SQL. If this were a SQL query, it would look something like this:\n","\n","```sql\n","SELECT CASE\n","       WHEN EDUCATION = '0' THEN 'Other'\n","       WHEN EDUCATION = '5' THEN 'Other'\n","       WHEN EDUCATION = '6' THEN 'Other'\n","       ELSE EDUCATION\n","       END AS EDUCATION\n","  FROM credit_card_default;\n","```\n","\n","With Spark SQL DataFrames, this is achieved using `.withColumn()` ([documentation here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.withColumn.html)) in conjunction with `.when()` ([documentation here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.when.html)) and `.otherwise()` ([documentation here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.otherwise.html))."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+\n","|  EDUCATION|\n","+-----------+\n","|High School|\n","|      Other|\n","|   Graduate|\n","|    College|\n","+-----------+\n","\n","+--------+\n","|MARRIAGE|\n","+--------+\n","|   Other|\n","| Married|\n","|  Single|\n","+--------+\n","\n"]}],"source":["from pyspark.sql import functions as F\n","# Bin EDUCATION categories\n","df_education_binned = spark_df.withColumn('EDUCATION',\n","                                          F.when(spark_df['EDUCATION'] == '0', 'Other')\\\n","                                          .when(spark_df['EDUCATION'] == '5', 'Other')\\\n","                                          .when(spark_df['EDUCATION'] == '6', 'Other')\\\n","                                          .otherwise(spark_df['EDUCATION'])\n","                                         )\n","# Bin MARRIAGE categories\n","df_all_binned = df_education_binned.withColumn('MARRIAGE',\n","                                               F.when(df_education_binned['MARRIAGE'] == '0', 'Other')\\\n","                                               .otherwise(df_education_binned['MARRIAGE'])\n","                                              )\n","\n","# Select and show all distinct categories for EDUCATION and MARRIGE again\n","df_all_binned.select('EDUCATION').distinct().show()\n","df_all_binned.select('MARRIAGE').distinct().show()"]},{"cell_type":"markdown","metadata":{},"source":["Let's also re-create the plots from earlier, now that the data has been binned:"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"ename":"ImportError","evalue":"Pandas >= 0.23.2 must be installed; however, it was not found.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[1;32mc:\\Users\\zethu\\Desktop\\Flatiron\\phase-4\\module-38\\machine-learning-with-spark\\index.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zethu/Desktop/Flatiron/phase-4/module-38/machine-learning-with-spark/index.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Plot EDUCATION\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zethu/Desktop/Flatiron/phase-4/module-38/machine-learning-with-spark/index.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df_all_binned\u001b[39m.\u001b[39;49mgroupBy(\u001b[39m'\u001b[39;49m\u001b[39mEDUCATION\u001b[39;49m\u001b[39m'\u001b[39;49m)\\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zethu/Desktop/Flatiron/phase-4/module-38/machine-learning-with-spark/index.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m              \u001b[39m.\u001b[39;49mcount()\\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zethu/Desktop/Flatiron/phase-4/module-38/machine-learning-with-spark/index.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m              \u001b[39m.\u001b[39;49mtoPandas()\\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zethu/Desktop/Flatiron/phase-4/module-38/machine-learning-with-spark/index.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m              \u001b[39m.\u001b[39mplot(x\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEDUCATION\u001b[39m\u001b[39m\"\u001b[39m, y\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m\"\u001b[39m, kind\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbar\u001b[39m\u001b[39m\"\u001b[39m, rot\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n","File \u001b[1;32mc:\\Users\\zethu\\anaconda3\\envs\\spark-env\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:63\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, DataFrame)\n\u001b[0;32m     62\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m require_minimum_pandas_version\n\u001b[1;32m---> 63\u001b[0m require_minimum_pandas_version()\n\u001b[0;32m     65\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\zethu\\anaconda3\\envs\\spark-env\\lib\\site-packages\\pyspark\\sql\\pandas\\utils.py:32\u001b[0m, in \u001b[0;36mrequire_minimum_pandas_version\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m     have_pandas \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m have_pandas:\n\u001b[1;32m---> 32\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPandas >= \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must be installed; however, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     33\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mit was not found.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m minimum_pandas_version)\n\u001b[0;32m     34\u001b[0m \u001b[39mif\u001b[39;00m LooseVersion(pandas\u001b[39m.\u001b[39m__version__) \u001b[39m<\u001b[39m LooseVersion(minimum_pandas_version):\n\u001b[0;32m     35\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPandas >= \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must be installed; however, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39myour version was \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (minimum_pandas_version, pandas\u001b[39m.\u001b[39m__version__))\n","\u001b[1;31mImportError\u001b[0m: Pandas >= 0.23.2 must be installed; however, it was not found."]}],"source":["# Plot EDUCATION\n","df_all_binned.groupBy('EDUCATION')\\\n","             .count()\\\n","             .toPandas()\\\n","             .plot(x=\"EDUCATION\", y=\"count\", kind=\"bar\", rot=0);"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"ename":"ImportError","evalue":"Pandas >= 0.23.2 must be installed; however, it was not found.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[1;32mc:\\Users\\zethu\\Desktop\\Flatiron\\phase-4\\module-38\\machine-learning-with-spark\\index.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zethu/Desktop/Flatiron/phase-4/module-38/machine-learning-with-spark/index.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Plot MARRIAGE\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zethu/Desktop/Flatiron/phase-4/module-38/machine-learning-with-spark/index.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df_all_binned\u001b[39m.\u001b[39;49mgroupBy(\u001b[39m'\u001b[39;49m\u001b[39mMARRIAGE\u001b[39;49m\u001b[39m'\u001b[39;49m)\\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zethu/Desktop/Flatiron/phase-4/module-38/machine-learning-with-spark/index.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m              \u001b[39m.\u001b[39;49mcount()\\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zethu/Desktop/Flatiron/phase-4/module-38/machine-learning-with-spark/index.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m              \u001b[39m.\u001b[39;49mtoPandas()\\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zethu/Desktop/Flatiron/phase-4/module-38/machine-learning-with-spark/index.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m              \u001b[39m.\u001b[39mplot(x\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMARRIAGE\u001b[39m\u001b[39m\"\u001b[39m, y\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m\"\u001b[39m, kind\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbar\u001b[39m\u001b[39m\"\u001b[39m, rot\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n","File \u001b[1;32mc:\\Users\\zethu\\anaconda3\\envs\\spark-env\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:63\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, DataFrame)\n\u001b[0;32m     62\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m require_minimum_pandas_version\n\u001b[1;32m---> 63\u001b[0m require_minimum_pandas_version()\n\u001b[0;32m     65\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\zethu\\anaconda3\\envs\\spark-env\\lib\\site-packages\\pyspark\\sql\\pandas\\utils.py:32\u001b[0m, in \u001b[0;36mrequire_minimum_pandas_version\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m     have_pandas \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m have_pandas:\n\u001b[1;32m---> 32\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPandas >= \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must be installed; however, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     33\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mit was not found.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m minimum_pandas_version)\n\u001b[0;32m     34\u001b[0m \u001b[39mif\u001b[39;00m LooseVersion(pandas\u001b[39m.\u001b[39m__version__) \u001b[39m<\u001b[39m LooseVersion(minimum_pandas_version):\n\u001b[0;32m     35\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPandas >= \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must be installed; however, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39myour version was \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (minimum_pandas_version, pandas\u001b[39m.\u001b[39m__version__))\n","\u001b[1;31mImportError\u001b[0m: Pandas >= 0.23.2 must be installed; however, it was not found."]}],"source":["# Plot MARRIAGE\n","df_all_binned.groupBy('MARRIAGE')\\\n","             .count()\\\n","             .toPandas()\\\n","             .plot(x=\"MARRIAGE\", y=\"count\", kind=\"bar\", rot=0);"]},{"cell_type":"markdown","metadata":{},"source":["Much better. Now, let's do a little more investigation into our target variable before diving into the machine learning aspect of this project."]},{"cell_type":"markdown","metadata":{},"source":["##  Class Balance Exploration\n","\n","Let's first look at the overall distribution of class balance of the `default` column (the target for our upcoming machine learning process). \n","\n","Create a bar plot to compare the number of defaults (`0`) vs. non-defaults (`1`). Consider customizing your plot labels as well, since `0` and `1` are not particularly understandable values."]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-----+\n","|default|count|\n","+-------+-----+\n","|      0|23364|\n","|      1| 6635|\n","+-------+-----+\n","\n"]},{"ename":"ImportError","evalue":"Pandas >= 0.23.2 must be installed; however, it was not found.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[1;32mc:\\Users\\zethu\\Desktop\\Flatiron\\phase-4\\module-38\\machine-learning-with-spark\\index.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zethu/Desktop/Flatiron/phase-4/module-38/machine-learning-with-spark/index.ipynb#X25sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Plot target data\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zethu/Desktop/Flatiron/phase-4/module-38/machine-learning-with-spark/index.ipynb#X25sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/zethu/Desktop/Flatiron/phase-4/module-38/machine-learning-with-spark/index.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m target_cats\u001b[39m.\u001b[39;49mtoPandas()\u001b[39m.\u001b[39mplot(x\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m'\u001b[39m, y\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m'\u001b[39m, kind\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbar\u001b[39m\u001b[39m'\u001b[39m, ax\u001b[39m=\u001b[39max, rot\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zethu/Desktop/Flatiron/phase-4/module-38/machine-learning-with-spark/index.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m ax\u001b[39m.\u001b[39mset_xlabel(\u001b[39m\"\u001b[39m\u001b[39mTarget\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zethu/Desktop/Flatiron/phase-4/module-38/machine-learning-with-spark/index.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m ax\u001b[39m.\u001b[39mset_xticklabels([\u001b[39m'\u001b[39m\u001b[39mDoes Not Default (0)\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mDefaults (1)\u001b[39m\u001b[39m'\u001b[39m])\n","File \u001b[1;32mc:\\Users\\zethu\\anaconda3\\envs\\spark-env\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:63\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, DataFrame)\n\u001b[0;32m     62\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m require_minimum_pandas_version\n\u001b[1;32m---> 63\u001b[0m require_minimum_pandas_version()\n\u001b[0;32m     65\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\zethu\\anaconda3\\envs\\spark-env\\lib\\site-packages\\pyspark\\sql\\pandas\\utils.py:32\u001b[0m, in \u001b[0;36mrequire_minimum_pandas_version\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m     have_pandas \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m have_pandas:\n\u001b[1;32m---> 32\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPandas >= \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must be installed; however, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     33\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mit was not found.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m minimum_pandas_version)\n\u001b[0;32m     34\u001b[0m \u001b[39mif\u001b[39;00m LooseVersion(pandas\u001b[39m.\u001b[39m__version__) \u001b[39m<\u001b[39m LooseVersion(minimum_pandas_version):\n\u001b[0;32m     35\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPandas >= \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must be installed; however, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39myour version was \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (minimum_pandas_version, pandas\u001b[39m.\u001b[39m__version__))\n","\u001b[1;31mImportError\u001b[0m: Pandas >= 0.23.2 must be installed; however, it was not found."]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["# Group and aggregate target data\n","import matplotlib.pyplot as plt\n","\n","# Group and aggregate target data\n","target_cats = df_all_binned.groupBy('default').count().orderBy('default')\n","target_cats.show()\n","\n","# Plot target data\n","fig, ax = plt.subplots()\n","target_cats.toPandas().plot(x='default', y='count', kind='bar', ax=ax, rot=0)\n","ax.set_xlabel(\"Target\")\n","ax.set_xticklabels(['Does Not Default (0)','Defaults (1)']);\n"]},{"cell_type":"markdown","metadata":{},"source":["Looks like we have a fairly imbalanced dataset.\n","\n","Let's also visualize the difference in default rate between males and females in this dataset. Group by both `default` and `SEX` and visualize the comparison."]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+------+-----+\n","|default|   SEX|count|\n","+-------+------+-----+\n","|      0|Female|14349|\n","|      0|  Male| 9015|\n","|      1|Female| 3762|\n","|      1|  Male| 2873|\n","+-------+------+-----+\n","\n"]},{"ename":"ImportError","evalue":"Pandas >= 0.23.2 must be installed; however, it was not found.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[1;32mc:\\Users\\zethu\\Desktop\\Flatiron\\phase-4\\module-38\\machine-learning-with-spark\\index.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zethu/Desktop/Flatiron/phase-4/module-38/machine-learning-with-spark/index.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Plot target and sex data\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zethu/Desktop/Flatiron/phase-4/module-38/machine-learning-with-spark/index.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zethu/Desktop/Flatiron/phase-4/module-38/machine-learning-with-spark/index.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m target_by_sex\u001b[39m.\u001b[39;49mtoPandas()\u001b[39m.\u001b[39mpivot(\u001b[39m'\u001b[39m\u001b[39mSEX\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m'\u001b[39m)\\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zethu/Desktop/Flatiron/phase-4/module-38/machine-learning-with-spark/index.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                \u001b[39m.\u001b[39mplot(kind\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbar\u001b[39m\u001b[39m'\u001b[39m, ax\u001b[39m=\u001b[39max, rot\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zethu/Desktop/Flatiron/phase-4/module-38/machine-learning-with-spark/index.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m ax\u001b[39m.\u001b[39mlegend(title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCount\u001b[39m\u001b[39m\"\u001b[39m, labels\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mDoes Not Default (0)\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mDefaults (1)\u001b[39m\u001b[39m'\u001b[39m])\n","File \u001b[1;32mc:\\Users\\zethu\\anaconda3\\envs\\spark-env\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:63\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, DataFrame)\n\u001b[0;32m     62\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m require_minimum_pandas_version\n\u001b[1;32m---> 63\u001b[0m require_minimum_pandas_version()\n\u001b[0;32m     65\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\zethu\\anaconda3\\envs\\spark-env\\lib\\site-packages\\pyspark\\sql\\pandas\\utils.py:32\u001b[0m, in \u001b[0;36mrequire_minimum_pandas_version\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m     have_pandas \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m have_pandas:\n\u001b[1;32m---> 32\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPandas >= \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must be installed; however, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     33\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mit was not found.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m minimum_pandas_version)\n\u001b[0;32m     34\u001b[0m \u001b[39mif\u001b[39;00m LooseVersion(pandas\u001b[39m.\u001b[39m__version__) \u001b[39m<\u001b[39m LooseVersion(minimum_pandas_version):\n\u001b[0;32m     35\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPandas >= \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must be installed; however, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39myour version was \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (minimum_pandas_version, pandas\u001b[39m.\u001b[39m__version__))\n","\u001b[1;31mImportError\u001b[0m: Pandas >= 0.23.2 must be installed; however, it was not found."]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["# Group and aggregate target and sex data\n","target_by_sex = df_all_binned.groupBy(['default', 'SEX']).count().orderBy(['default', 'SEX'])\n","target_by_sex.show()\n","\n","# Plot target and sex data\n","fig, ax = plt.subplots()\n","\n","target_by_sex.toPandas().pivot('SEX', 'default')\\\n","               .plot(kind='bar', ax=ax, rot=0)\n","\n","ax.legend(title=\"Count\", labels=['Does Not Default (0)','Defaults (1)']);\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["It looks like males have an ever so slightly higher default rate than females, and also represent a smaller proportion of the dataset."]},{"cell_type":"markdown","metadata":{},"source":["## On to the Machine Learning!\n","\n","Now, it's time to fit the data to the PySpark machine learning model pipeline. You will need:\n","\n","* 3 `StringIndexer`s\n","  * One for each categorical feature\n","  * [Documentation here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.StringIndexer.html)\n","* A `OneHotEncoder`\n","  * To encode the newly indexed strings into categorical variables\n","  * [Documentation here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.OneHotEncoder.html)\n","* A `VectorAssembler`\n","  * To combine all features into one `SparseVector`\n","  * [Documentation here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html)\n","\n","All of these initialized estimators should be stored in a list called `stages`."]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Indexed string features: ['SEX_num', 'EDUCATION_num', 'MARRIAGE_num'] \n","\n","Numeric features: ['LIMIT_BAL', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6'] \n","\n","Final features: ['LIMIT_BAL', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6', 'SEX_num_ohe', 'EDUCATION_num_ohe', 'MARRIAGE_num_ohe'] \n","\n","Stages: [StringIndexer_cd79817e37b8, StringIndexer_2cdf155a20e4, StringIndexer_017479b0e25d, OneHotEncoder_63d1394ea281, VectorAssembler_5601283c3fd1]\n"]}],"source":["# Import the necessary classes\n","from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n","\n","# Create the string indexers and determine the names of the numeric\n","# and indexed columns\n","# (You could write this out for all 3 but we'll use a loop approach)\n","indexers = []\n","indexed_features = []\n","numeric_features = []\n","\n","for column, data_type in df_all_binned.dtypes:\n","    if data_type == 'string':\n","        # Create StringIndexers for columns containing strings\n","        si = StringIndexer(inputCol=column, outputCol=column+'_num', handleInvalid='keep')\n","        indexers.append(si)\n","        # Save the name of the output column to sent to the OHE\n","        indexed_features.append(si.getOutputCol())\n","    elif column != \"ID\" and column != \"default\":\n","        # Unless it's ID (an identifier rather than a genuine feature),\n","        # append to list of numeric features if the dtype isn't string\n","        numeric_features.append(column)\n","\n","print(\"Indexed string features:\", indexed_features, \"\\n\")\n","print(\"Numeric features:\", numeric_features, \"\\n\")\n","\n","# Create a OneHotEncoder to encode the indexed string features\n","ohe = OneHotEncoder(\n","    inputCols=indexed_features,\n","    outputCols=[col + '_ohe' for col in indexed_features],\n","    dropLast=True\n",")\n","\n","# Determine the names of the final list of features going into the model\n","features = numeric_features + ohe.getOutputCols()\n","print(\"Final features:\", features, \"\\n\")\n","\n","# Create a VectorAssembler to combine all features\n","va = VectorAssembler(inputCols=features , outputCol='features')\n","\n","# Assemble a list of stages that includes all indexers, the one-hot\n","# encoder, and the vector assembler\n","stages = indexers + [ohe, va]\n","print(\"Stages:\", stages)"]},{"cell_type":"markdown","metadata":{},"source":["Great! Now let's see if that worked. Let's investigate how it transforms your dataset. Put all of the stages in a Pipeline and fit it to your data. Look at the features column. Did you obtain the number of features you expected?"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["29"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# Import relevant class\n","from pyspark.ml.pipeline import Pipeline\n","\n","# Instantiate a pipeline using stages list\n","pipe = Pipeline(stages=stages)\n","\n","# Fit and transform the data using the pipeline, then look at\n","# the size of the array in the 'features' column\n","pipe.fit(df_all_binned)\\\n","    .transform(df_all_binned)\\\n","    .head()['features'].size\n"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary><u>Click to reveal answer</u></summary>\n","    \n","The pipeline should have produced a sparse vector with 29 features.\n","\n","This comes from:\n","    \n","* 20 numeric features\n","* 3 one-hot encoded features with `dropLast=True`, containing\n","  * 1 SEX feature\n","  * 3 EDUCATION features\n","  * 2 MARRIAGE features\n","\n","</details>"]},{"cell_type":"markdown","metadata":{},"source":["## Fitting Machine Learning Models\n","That looks good! Now let's go ahead and fit data to different machine learning models. To evaluate these models, you should use the `BinaryClassificationEvaluator`."]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","evaluator = BinaryClassificationEvaluator(\n","    rawPredictionCol='prediction',\n","    labelCol='default',\n","    metricName='areaUnderROC'\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Logistic Regression\n","\n","First, we'll try a `LogisticRegression` ([documentation here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.LogisticRegression.html)):\n","\n","* split the data into a train and test set. The basic structure of this is:\n","```\n","train, test = df.randomSplit(weights=[0.8, 0.2], seed=1)\n","```\n","  * make sure you replace `df` with the actual name of your prepared dataframe\n","* instantiate a logistic regression with `standardization=True` and add it to the stages list\n","* instantiate a new Pipeline estimator with all of the stages\n","* fit the pipeline on the training data\n","* transform both train and test data using the pipeline\n","* use `evaluator` to evaluate performance on train vs. test"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ROC-AUC on train: 0.6057796783170918\n","ROC-AUC on test: 0.6090136625813551\n"]}],"source":["from pyspark.ml.classification import LogisticRegression\n","\n","# Train-test split\n","train, test = df_all_binned.randomSplit(weights=[0.8, 0.2], seed=1)\n","\n","# Instantiate a logistic regression and add to stages\n","lr = LogisticRegression(\n","    featuresCol='features',\n","    labelCol='default',\n","    predictionCol='prediction',\n","    standardization=True\n",")\n","lr_stages = stages + [lr]\n","\n","# Instantiate a new Pipeline with all of the stages\n","lr_pipe = Pipeline(stages=lr_stages)\n","\n","# Fit the pipeline on the training data\n","lr_pipe_fit = lr_pipe.fit(train)\n","\n","# Transform both train and test data using the pipeline\n","lr_result_train = lr_pipe_fit.transform(train)\n","lr_result_test = lr_pipe_fit.transform(test)\n","\n","# Use evaluator to evaluate performance\n","print(\"ROC-AUC on train:\", evaluator.evaluate(lr_result_train))\n","print(\"ROC-AUC on test:\", evaluator.evaluate(lr_result_test))\n"]},{"cell_type":"markdown","metadata":{},"source":["Looks like the defaults for `LogisticRegression` are working pretty well, since the train and test metrics are pretty similar.\n","\n","Still, let's try a `CrossValidator` ([documentation here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.CrossValidator.html)) + `ParamGridBuilder` ([documentation here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.ParamGridBuilder.html)) approach with a few different regularization parameters.\n","\n","We'll use these regularization parameters:\n","\n","```python\n","[0.0, 0.01, 0.1, 1.0]\n","```\n","\n","In the cell below:\n","\n","* instantiate a `ParamGridBuilder` that tests out the `regParam` values listed above\n","* instantiate a `CrossValidator` that uses the param grid you just created as well as `evaluator` and the pipeline you created earlier\n","* fit the `CrossValidator` on the full DataFrame\n","* display the metrics for all models, and identify the best model parameters"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n","\n","# Set up param grid\n","lr_params = ParamGridBuilder().addGrid(lr.regParam, [0.0, 0.01, 0.1, 1.0]).build()\n","\n","# Set up cross validator\n","lr_cv = CrossValidator(\n","    estimator=lr_pipe,\n","    estimatorParamMaps=lr_params,\n","    evaluator=evaluator\n",")\n","\n","# Fit cross validator on the full dataframe\n","lr_model = lr_cv.fit(df_all_binned)"]},{"cell_type":"markdown","metadata":{},"source":["Now try this again with other classifiers. Try and create a function that will allow you to easily test different models with different parameters. You can find all of the available classification model options [here](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html#classification).\n","\n","This function is optional, but it should allow for your code to be far more D.R.Y. The function should return the fitted cross-validated classifier as well as print out the AUC of the best-performing model and the best parameters."]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["# Create a function to cross validate different classifiers with different parameters\n","\n","import numpy as np\n","\n","def create_model(df, preprocessing_stages, classifier, param_grid, evaluator):\n","    # Fit the cross validator model\n","    stages_with_classifier = preprocessing_stages + [classifier]\n","    pipe = Pipeline(stages=stages_with_classifier)\n","    cv = CrossValidator(\n","        estimator=pipe,\n","        estimatorParamMaps=param_grid,\n","        evaluator=evaluator\n","    )\n","    model = cv.fit(df)\n","    \n","    # Find and display the best classifier's metric and params\n","    index_best_clf = np.argmax(model.avgMetrics)\n","    best_clf_metric = model.avgMetrics[index_best_clf]\n","    best_clf_params = param_grid[index_best_clf]\n","    print(f\"\"\"\n","Best {evaluator.getMetricName()}: {best_clf_metric}\n","\n","Best params: {best_clf_params}\n","    \"\"\")\n","    \n","    # Return the model\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["Now train one other classifier that is not a `LogisticRegression`. Use a `ParamGridBuilder` to try out some relevant parameters."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Your code here\n","#  This cell may take a long time to run\n","from pyspark.ml.classification import RandomForestClassifier\n","rf = RandomForestClassifier(featuresCol='features',labelCol='default')\n","rf_params = ParamGridBuilder()\\\n"," .addGrid(rf.maxDepth, [5,10])\\\n"," .addGrid(rf.numTrees, [20,100])\\\n"," .build()\n","\n","rf_model = create_model(df_all_binned, stages, rf, rf_params, evaluator)"]},{"cell_type":"markdown","metadata":{},"source":["And one more:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Your code here\n","#  This cell may take a long time to run\n","from pyspark.ml.classification import GBTClassifier\n","gb = GBTClassifier(featuresCol='features',labelCol='default')\n","gb_params = ParamGridBuilder()\\\n","  .addGrid(gb.maxDepth,[1,5])\\\n","  .addGrid(gb.maxIter,[20,50])\\\n","  .build()\n","\n","gb_model = create_model(df_all_binned, stages, gb, gb_params, evaluator)"]},{"cell_type":"markdown","metadata":{},"source":["Which classifier turned out to be the best overall?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Your answer here\n","\"\"\"\n","\n","\"\"\";"]},{"cell_type":"markdown","metadata":{},"source":["## Level Up (Optional)\n","\n","* Create ROC curves for each of these models\n","* Try the multi-layer perceptron classifier algorithm. You will soon learn about what this means in the neural network section!"]},{"cell_type":"markdown","metadata":{},"source":["## Stop the Spark Session"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["spark.stop()"]},{"cell_type":"markdown","metadata":{},"source":["## Summary\n","\n","If you've made it this far, congratulations! Spark is an in-demand skill, but it is not particularly easy to master. In this lesson, you fit multiple different machine learning pipelines for a classification problem. If you want to take your Spark skills to the next level, connect to a distributed cluster using a service like AWS or Databricks and perform these Spark operations on the cloud."]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.13 ('spark-env')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"6fe6feec2256f8b0356aa2a056f58eeec805543f276c4d0a5e73d95b8c4d7bc8"}}},"nbformat":4,"nbformat_minor":2}

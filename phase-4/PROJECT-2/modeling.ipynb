{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependences for data loading and sifting\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths to data directories\n",
    "train_dir = \"data/train/\"\n",
    "test_dir = \"data/test/\"\n",
    "val_dir = \"data/val/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n",
      "Found 16 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# instatiating a data degenerater for each split sample \n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "                                   \n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_data_generator = train_datagen.flow_from_directory(\n",
    "                       train_dir,\n",
    "                       target_size=(150,150),\n",
    "                       batch_size=16,\n",
    "                       class_mode='binary',\n",
    "                       color_mode='grayscale')\n",
    "\n",
    "test_data_generator = test_datagen.flow_from_directory(\n",
    "                      test_dir,\n",
    "                      target_size=(150,150),\n",
    "                      batch_size=16,\n",
    "                      class_mode='binary',\n",
    "                      color_mode='grayscale')\n",
    "\n",
    "val_data_generator = val_datagen.flow_from_directory(\n",
    "                     val_dir,\n",
    "                     target_size=(150,150),\n",
    "                     batch_size=16,\n",
    "                     class_mode='binary',\n",
    "                     color_mode='grayscale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "Beginning a convolutional neural network with the most simple architecture for the sake of efficient diagnostics and optimization.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture begins with a convolutional 2D layers, followed by a pooling layer, flattened, and then followed up with two dense layers.  \n",
    "THe model is compiled using binary cross entropy to measure the loss function and accuracy foor the performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your code here; design and compile the model\n",
    "base_model = models.Sequential()\n",
    "base_model.add(layers.Conv2D(32,(3,3),activation='relu',input_shape=(150,150,1)))\n",
    "base_model.add(layers.MaxPooling2D((2, 2)))\n",
    "base_model.add(layers.Flatten())\n",
    "base_model.add(layers.Dense(512, activation='relu'))\n",
    "base_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "base_model.compile(loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the compiled base model is now fit very lean, using 10 steps per epoch and 5 total epochs, the test data set is used for validation as the actual validation sample is extremely small and reserved only for the final phase of performance evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10/10 [==============================] - 16s 2s/step - loss: 2.3194 - acc: 0.7625 - val_loss: 0.6574 - val_acc: 0.7333\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.4071 - acc: 0.8500 - val_loss: 0.4250 - val_acc: 0.8333\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 11s 1s/step - loss: 0.2486 - acc: 0.8938 - val_loss: 0.5820 - val_acc: 0.7458\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 11s 1s/step - loss: 0.3312 - acc: 0.8562 - val_loss: 0.8100 - val_acc: 0.7458\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 10s 1s/step - loss: 0.1977 - acc: 0.9250 - val_loss: 0.4881 - val_acc: 0.7958\n"
     ]
    }
   ],
   "source": [
    "base_model_results = base_model.fit(train_data_generator, \n",
    "                              steps_per_epoch=10, \n",
    "                              epochs=5, \n",
    "                              validation_data=test_data_generator, \n",
    "                              validation_steps=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base model begins stronger than expected with an accuracy of around 70% on the test data (sometimes better or worse, re-running the cell multiple times). A good start, with plenty of room to improve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sophisticating model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before exploring tuning and optimization of hyperparameters and other methods such as regularization, we will sophisticate the architectuer of the model itself by incorporating more layers in a variety of types including convolutional layers, pooling layers, and dense layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_model = models.Sequential()\n",
    "\n",
    "deep_model.add(layers.Conv2D(32,(3,3),activation='relu',input_shape=(150,150,1)))\n",
    "deep_model.add(layers.MaxPooling2D((2, 2)))\n",
    "deep_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "deep_model.add(layers.MaxPooling2D((2, 2)))\n",
    "deep_model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "deep_model.add(layers.MaxPooling2D((2, 2)))\n",
    "deep_model.add(layers.Flatten())\n",
    "deep_model.add(layers.Dense(512, activation='relu'))\n",
    "deep_model.add(layers.Dense(256, activation='relu'))\n",
    "deep_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "deep_model.compile(loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10/10 [==============================] - 8s 819ms/step - loss: 2.0752 - acc: 0.6375 - val_loss: 0.7143 - val_acc: 0.6042\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 8s 825ms/step - loss: 0.5867 - acc: 0.7500 - val_loss: 1.2636 - val_acc: 0.6250\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 8s 802ms/step - loss: 0.6839 - acc: 0.7063 - val_loss: 0.4876 - val_acc: 0.8125\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 8s 755ms/step - loss: 0.9015 - acc: 0.7563 - val_loss: 0.5659 - val_acc: 0.6667\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 9s 912ms/step - loss: 0.4580 - acc: 0.8062 - val_loss: 0.5865 - val_acc: 0.6042\n"
     ]
    }
   ],
   "source": [
    "deep_model_results = deep_model.fit(train_data_generator, \n",
    "                              steps_per_epoch=10, \n",
    "                              epochs=5, \n",
    "                              validation_data=test_data_generator, \n",
    "                              validation_steps=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('learn-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8eb6b2671ae257969e1e6d572d1bfd2dc1c23f390a7081d26c146f9b82ef978d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 39 Topic Review: Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization & Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg,stopwords\n",
    "from nltk.collocations import *\n",
    "from nltk import FreqDist\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = \"\"\"\n",
    "It’s a marvelous day for a moon launch\n",
    "NASA rocket in FloridaJoe Raedle/Getty Images\n",
    "Did you know that a human hasn’t stepped foot on the moon since 1972?\n",
    "\n",
    "Well, NASA wants to change that, and today marks a major milestone on its quest to put astronauts back on the lunar surface. At 8:33am ET, the agency is set to launch its new moon rocket called the Space Launch System (SLS), the most powerful rocket ever built. The uncrewed mission will head toward the moon and complete one and a half orbits during its 42-day mission.\n",
    "\n",
    "If successful, the voyage will set the stage for a crewed “flyby” mission to the moon in 2024, teeing up a potential landing as soon as the following year.\n",
    "\n",
    "But this is rocket science after all, and most things don’t go according to plan. The SLS was first ordered by Congress in 2010, and is only now hitting the launchpad after numerous delays and billions in cost overruns. Plus, getting humans on the moon will require not only the rocket but also a vehicle to send astronauts from the capsule to the moon’s surface. SpaceX has been tapped to provide that lunar lander, though it hasn’t successfully reached orbit yet.\n",
    "\n",
    "“I would say simply that space is hard,” NASA Administrator Bill Nelson deadpanned on Saturday.\n",
    "\n",
    "So why even go to the moon?\n",
    "Here are a few reasons why NASA thinks it’s worth the trouble, per NPR.\n",
    "\n",
    "Science: Lunar geologists say some parts of the moon are pivotal for understanding the beginnings of the solar system, because there’s no atmosphere or flowing water to erode rocks.\n",
    "Dress rehearsal for Mars: Before astronauts head to the Red Planet, they can work out all the kinks on the moon, which is 200 times closer to Earth than Mars.\n",
    "Marketing: Hey, we’re talking about NASA, right? Doing big, buzzy projects like moon landings could help boost the reputation of the agency and also inspire more Americans to pursue science and engineering careers.\n",
    "Final fun fact: NASA’s new moon program is called Artemis. In Greek myth, Artemis was the twin sister of Apollo, the name of NASA’s OG moon program.\n",
    "\"\"\"\n",
    "\n",
    "world = \"\"\"\n",
    "Tour de headlines\n",
    "Flooded street in PakistanAkram Shahid/AFP via Getty Images\n",
    " Pakistan faces a “climate catastrophe.” Pakistan officials said on Sunday that flooding from monsoon season has killed more than 1,000 people since mid-June, including 119 in the previous 24 hours. Flash floods that have destroyed villages and affected at least 33 million people amounted to a “climate-induced humanitarian disaster of epic proportions,” Pakistan’s climate change minister said.\n",
    "\n",
    " $3 movie tickets are coming. You’ll be able to tell your grandkids, “Back in my day, I paid $3 for a movie ticket,” because on Saturday the majority of movie theaters in the US will sell tickets for $3. The National Cinema Day initiative, launched by the nonprofit Cinema Foundation, is an effort to get people back into theaters during Labor Day weekend, which is typically one of the weakest all year. Plus, domestic box office sales this summer are still lagging 2019 levels by 20%.\n",
    "\n",
    " Baseball card sells for $12.6 million. A 1952 Mickey Mantle baseball card in mint condition sold for $12.6 million at auction—topping a $9.3 million Diego Maradona jersey as the most expensive piece of sports memorabilia ever sold. The sports collectibles space has exploded in popularity during the pandemic. In 2018, the size of the market was estimated to be around $5.4 million. By 2021, it grew to $26 billion.\n",
    "\"\"\"\n",
    "\n",
    "education = \"\"\"\n",
    "Missouri school district brings back spanking\n",
    "Tom spanking Jerry in the cartoonTom and Jerry/Warner Bros. via Giphy\n",
    "If all other disciplinary actions fail, teachers in Missouri’s Cassville R-IV School District will be allowed to spank a student with a paddle, parents learned last week. The superintendent said the school board brought back the practice after parents asked for more punishments other than suspension.\n",
    "\n",
    "The parent has to approve the spanking of their child, but once they do, a teacher can use “reasonable physical force” on a student but give no “chance of bodily injury or harm.” A witness has to be present during the spanking and a teacher or principal must give notice to the superintendent justifying the punishment.\n",
    "\n",
    "Big picture: You may be surprised to learn that corporal punishment—as it’s formally known—is legal in 19 states. That’s because a Supreme Court decision in 1977 said the technique was constitutional and let each state decide on its own rules.\n",
    "\n",
    "More than 69,000 children were punished physically in the 2017–2018 school year, according to the most recent data.\n",
    "\n",
    "Spanking in schools has loads of critics, including the American Academy of Pediatrics and American Psychological Association, which say it’s not effective and can give students trauma. A 2016 study found that boys, Black kids, and children with disabilities were more likely to be paddled than their peers.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " 's',\n",
       " 'a',\n",
       " 'marvelous',\n",
       " 'day',\n",
       " 'for',\n",
       " 'a',\n",
       " 'moon',\n",
       " 'launch',\n",
       " 'NASA',\n",
       " 'rocket',\n",
       " 'in',\n",
       " 'FloridaJoe',\n",
       " 'Raedle',\n",
       " 'Getty',\n",
       " 'Images',\n",
       " 'Did',\n",
       " 'you',\n",
       " 'know',\n",
       " 'that',\n",
       " 'a',\n",
       " 'human',\n",
       " 'hasn',\n",
       " 't',\n",
       " 'stepped',\n",
       " 'foot',\n",
       " 'on',\n",
       " 'the',\n",
       " 'moon',\n",
       " 'since',\n",
       " 'Well',\n",
       " 'NASA',\n",
       " 'wants',\n",
       " 'to',\n",
       " 'change',\n",
       " 'that',\n",
       " 'and',\n",
       " 'today',\n",
       " 'marks',\n",
       " 'a',\n",
       " 'major',\n",
       " 'milestone',\n",
       " 'on',\n",
       " 'its',\n",
       " 'quest',\n",
       " 'to',\n",
       " 'put',\n",
       " 'astronauts',\n",
       " 'back',\n",
       " 'on',\n",
       " 'the',\n",
       " 'lunar',\n",
       " 'surface',\n",
       " 'At',\n",
       " 'am',\n",
       " 'ET',\n",
       " 'the',\n",
       " 'agency',\n",
       " 'is',\n",
       " 'set',\n",
       " 'to',\n",
       " 'launch',\n",
       " 'its',\n",
       " 'new',\n",
       " 'moon',\n",
       " 'rocket',\n",
       " 'called',\n",
       " 'the',\n",
       " 'Space',\n",
       " 'Launch',\n",
       " 'System',\n",
       " 'SLS',\n",
       " 'the',\n",
       " 'most',\n",
       " 'powerful',\n",
       " 'rocket',\n",
       " 'ever',\n",
       " 'built',\n",
       " 'The',\n",
       " 'uncrewed',\n",
       " 'mission',\n",
       " 'will',\n",
       " 'head',\n",
       " 'toward',\n",
       " 'the',\n",
       " 'moon',\n",
       " 'and',\n",
       " 'complete',\n",
       " 'one',\n",
       " 'and',\n",
       " 'a',\n",
       " 'half',\n",
       " 'orbits',\n",
       " 'during',\n",
       " 'its',\n",
       " 'day',\n",
       " 'mission',\n",
       " 'If',\n",
       " 'successful',\n",
       " 'the',\n",
       " 'voyage',\n",
       " 'will',\n",
       " 'set',\n",
       " 'the',\n",
       " 'stage',\n",
       " 'for',\n",
       " 'a',\n",
       " 'crewed',\n",
       " 'flyby',\n",
       " 'mission',\n",
       " 'to',\n",
       " 'the',\n",
       " 'moon',\n",
       " 'in',\n",
       " 'teeing',\n",
       " 'up',\n",
       " 'a',\n",
       " 'potential',\n",
       " 'landing',\n",
       " 'as',\n",
       " 'soon',\n",
       " 'as',\n",
       " 'the',\n",
       " 'following',\n",
       " 'year',\n",
       " 'But',\n",
       " 'this',\n",
       " 'is',\n",
       " 'rocket',\n",
       " 'science',\n",
       " 'after',\n",
       " 'all',\n",
       " 'and',\n",
       " 'most',\n",
       " 'things',\n",
       " 'don',\n",
       " 't',\n",
       " 'go',\n",
       " 'according',\n",
       " 'to',\n",
       " 'plan',\n",
       " 'The',\n",
       " 'SLS',\n",
       " 'was',\n",
       " 'first',\n",
       " 'ordered',\n",
       " 'by',\n",
       " 'Congress',\n",
       " 'in',\n",
       " 'and',\n",
       " 'is',\n",
       " 'only',\n",
       " 'now',\n",
       " 'hitting',\n",
       " 'the',\n",
       " 'launchpad',\n",
       " 'after',\n",
       " 'numerous',\n",
       " 'delays',\n",
       " 'and',\n",
       " 'billions',\n",
       " 'in',\n",
       " 'cost',\n",
       " 'overruns',\n",
       " 'Plus',\n",
       " 'getting',\n",
       " 'humans',\n",
       " 'on',\n",
       " 'the',\n",
       " 'moon',\n",
       " 'will',\n",
       " 'require',\n",
       " 'not',\n",
       " 'only',\n",
       " 'the',\n",
       " 'rocket',\n",
       " 'but',\n",
       " 'also',\n",
       " 'a',\n",
       " 'vehicle',\n",
       " 'to',\n",
       " 'send',\n",
       " 'astronauts',\n",
       " 'from',\n",
       " 'the',\n",
       " 'capsule',\n",
       " 'to',\n",
       " 'the',\n",
       " 'moon',\n",
       " 's',\n",
       " 'surface',\n",
       " 'SpaceX',\n",
       " 'has',\n",
       " 'been',\n",
       " 'tapped',\n",
       " 'to',\n",
       " 'provide',\n",
       " 'that',\n",
       " 'lunar',\n",
       " 'lander',\n",
       " 'though',\n",
       " 'it',\n",
       " 'hasn',\n",
       " 't',\n",
       " 'successfully',\n",
       " 'reached',\n",
       " 'orbit',\n",
       " 'yet',\n",
       " 'I',\n",
       " 'would',\n",
       " 'say',\n",
       " 'simply',\n",
       " 'that',\n",
       " 'space',\n",
       " 'is',\n",
       " 'hard',\n",
       " 'NASA',\n",
       " 'Administrator',\n",
       " 'Bill',\n",
       " 'Nelson',\n",
       " 'deadpanned',\n",
       " 'on',\n",
       " 'Saturday',\n",
       " 'So',\n",
       " 'why',\n",
       " 'even',\n",
       " 'go',\n",
       " 'to',\n",
       " 'the',\n",
       " 'moon',\n",
       " 'Here',\n",
       " 'are',\n",
       " 'a',\n",
       " 'few',\n",
       " 'reasons',\n",
       " 'why',\n",
       " 'NASA',\n",
       " 'thinks',\n",
       " 'it',\n",
       " 's',\n",
       " 'worth',\n",
       " 'the',\n",
       " 'trouble',\n",
       " 'per',\n",
       " 'NPR',\n",
       " 'Science',\n",
       " 'Lunar',\n",
       " 'geologists',\n",
       " 'say',\n",
       " 'some',\n",
       " 'parts',\n",
       " 'of',\n",
       " 'the',\n",
       " 'moon',\n",
       " 'are',\n",
       " 'pivotal',\n",
       " 'for',\n",
       " 'understanding',\n",
       " 'the',\n",
       " 'beginnings',\n",
       " 'of',\n",
       " 'the',\n",
       " 'solar',\n",
       " 'system',\n",
       " 'because',\n",
       " 'there',\n",
       " 's',\n",
       " 'no',\n",
       " 'atmosphere',\n",
       " 'or',\n",
       " 'flowing',\n",
       " 'water',\n",
       " 'to',\n",
       " 'erode',\n",
       " 'rocks',\n",
       " 'Dress',\n",
       " 'rehearsal',\n",
       " 'for',\n",
       " 'Mars',\n",
       " 'Before',\n",
       " 'astronauts',\n",
       " 'head',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Red',\n",
       " 'Planet',\n",
       " 'they',\n",
       " 'can',\n",
       " 'work',\n",
       " 'out',\n",
       " 'all',\n",
       " 'the',\n",
       " 'kinks',\n",
       " 'on',\n",
       " 'the',\n",
       " 'moon',\n",
       " 'which',\n",
       " 'is',\n",
       " 'times',\n",
       " 'closer',\n",
       " 'to',\n",
       " 'Earth',\n",
       " 'than',\n",
       " 'Mars',\n",
       " 'Marketing',\n",
       " 'Hey',\n",
       " 'we',\n",
       " 're',\n",
       " 'talking',\n",
       " 'about',\n",
       " 'NASA',\n",
       " 'right',\n",
       " 'Doing',\n",
       " 'big',\n",
       " 'buzzy',\n",
       " 'projects',\n",
       " 'like',\n",
       " 'moon',\n",
       " 'landings',\n",
       " 'could',\n",
       " 'help',\n",
       " 'boost',\n",
       " 'the',\n",
       " 'reputation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'agency',\n",
       " 'and',\n",
       " 'also',\n",
       " 'inspire',\n",
       " 'more',\n",
       " 'Americans',\n",
       " 'to',\n",
       " 'pursue',\n",
       " 'science',\n",
       " 'and',\n",
       " 'engineering',\n",
       " 'careers',\n",
       " 'Final',\n",
       " 'fun',\n",
       " 'fact',\n",
       " 'NASA',\n",
       " 's',\n",
       " 'new',\n",
       " 'moon',\n",
       " 'program',\n",
       " 'is',\n",
       " 'called',\n",
       " 'Artemis',\n",
       " 'In',\n",
       " 'Greek',\n",
       " 'myth',\n",
       " 'Artemis',\n",
       " 'was',\n",
       " 'the',\n",
       " 'twin',\n",
       " 'sister',\n",
       " 'of',\n",
       " 'Apollo',\n",
       " 'the',\n",
       " 'name',\n",
       " 'of',\n",
       " 'NASA',\n",
       " 's',\n",
       " 'OG',\n",
       " 'moon',\n",
       " 'program']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "space_tokens_raw = nltk.regexp_tokenize(space,pattern)\n",
    "space_tokens_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 29, 'moon': 13, 'to': 13, 'a': 9, 'and': 8, 'nasa': 7, 's': 6, 'on': 6, 'is': 6, 'rocket': 5, ...})"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space_tokens = [token.lower() for token in space_tokens_raw]\n",
    "space_FreqDist = FreqDist(space_tokens)\n",
    "space_FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words\n",
    "Stop words are common words found in nearly every corpus, and contribute little to the meaning or context of piece of natural language data (e.g. a phrase, text document, etc.), sometimes numbers are included here as well. Stop words are usually removed from the corpus when vectorizing a text document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus size:  157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FreqDist({'moon': 13, 'nasa': 7, 'rocket': 5, 'launch': 3, 'astronauts': 3, 'lunar': 3, 'mission': 3, 'science': 3, 'day': 2, 'surface': 2, ...})"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list += list(string.punctuation)\n",
    "stopwords_list += ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "space_stopped = [word for word in space_tokens if word not in stopwords_list]\n",
    "space_stopped_FreqDist = FreqDist(space_stopped)\n",
    "print(\"corpus size: \",len(space_stopped_FreqDist))\n",
    "space_stopped_FreqDist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency & Inverse Document Frequency TF-IDF\n",
    "\n",
    "$$TF(t)= \\frac{\\text{number of times t appears in a document}}{\\text{total number of terms in the document}}$$\n",
    "\n",
    "$$IDF(t) = log_{e}\\big(\\frac{\\text{total number of documents}}{\\text{number of documents with t in it}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_digest(text_doc:str):\n",
    "    pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "    tokens_raw = nltk.regexp_tokenize(text_doc,pattern)\n",
    "    lowered_tokens = [token.lower() for token in tokens_raw]\n",
    "    \n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list += list(string.punctuation)\n",
    "    stopwords_list += ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    stopped = [word for word in lowered_tokens if word not in stopwords_list]\n",
    "    \n",
    "    bigram_measrues = nltk.collocations.BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(stopped)\n",
    "    \n",
    "    scored = finder.score_ngrams(bigram_measrues.raw_freq)\n",
    "    freq_dist = FreqDist(stopped)\n",
    "\n",
    "    return freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpi = [corpus_digest(space),corpus_digest(world),corpus_digest(education)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count total number of instances for each word accross all documents\n",
    "big_corpus_tf = {word: 0 for word in (list(corpi[0].keys())+list(corpi[1].keys())+list(corpi[2].keys()))}\n",
    "for word in big_corpus_tf.keys():\n",
    "    for corpus in corpi:\n",
    "        big_corpus_tf[word] += corpus[word]\n",
    "\n",
    "\n",
    "# count number of documents each word occurs in \n",
    "big_corpus_doc_freq = {}\n",
    "for word in big_corpus_tf:\n",
    "    num_of_docs = 0\n",
    "    for dict in corpi:\n",
    "        if word in dict.keys():\n",
    "            num_of_docs += 1 \n",
    "    big_corpus_doc_freq[word] = num_of_docs\n",
    "\n",
    "\n",
    "# calculate term frequency and IDF for each document\n",
    "tf_dicts = []\n",
    "idf_dicts = []\n",
    "for corpus in corpi:\n",
    "    tf_dicts.append({word:(corpus[word]/len(corpus.keys())) for word in corpus})\n",
    "for corpus in corpi:\n",
    "    idf_dicts.append({word:(3/big_corpus_doc_freq[word]) for word in corpus})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_tf_idf = {word:(tf_dicts[0][word],idf_dicts[0][word]) for word in corpi[0].keys()}\n",
    "world_tf_idf = {word:(tf_dicts[1][word],idf_dicts[1][word]) for word in corpi[1].keys()}\n",
    "education_tf_idf = {word:(tf_dicts[2][word],idf_dicts[2][word]) for word in corpi[2].keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'marvelous': (0.006369426751592357, 3.0),\n",
       " 'day': (0.012738853503184714, 1.5),\n",
       " 'moon': (0.08280254777070063, 3.0),\n",
       " 'launch': (0.01910828025477707, 3.0),\n",
       " 'nasa': (0.044585987261146494, 3.0),\n",
       " 'rocket': (0.03184713375796178, 3.0),\n",
       " 'floridajoe': (0.006369426751592357, 3.0),\n",
       " 'raedle': (0.006369426751592357, 3.0),\n",
       " 'getty': (0.006369426751592357, 1.5),\n",
       " 'images': (0.006369426751592357, 1.5),\n",
       " 'know': (0.006369426751592357, 3.0),\n",
       " 'human': (0.006369426751592357, 3.0),\n",
       " 'stepped': (0.006369426751592357, 3.0),\n",
       " 'foot': (0.006369426751592357, 3.0),\n",
       " 'since': (0.006369426751592357, 1.5),\n",
       " 'well': (0.006369426751592357, 3.0),\n",
       " 'wants': (0.006369426751592357, 3.0),\n",
       " 'change': (0.006369426751592357, 1.5),\n",
       " 'today': (0.006369426751592357, 3.0),\n",
       " 'marks': (0.006369426751592357, 3.0),\n",
       " 'major': (0.006369426751592357, 3.0),\n",
       " 'milestone': (0.006369426751592357, 3.0),\n",
       " 'quest': (0.006369426751592357, 3.0),\n",
       " 'put': (0.006369426751592357, 3.0),\n",
       " 'astronauts': (0.01910828025477707, 3.0),\n",
       " 'back': (0.006369426751592357, 1.0),\n",
       " 'lunar': (0.01910828025477707, 3.0),\n",
       " 'surface': (0.012738853503184714, 3.0),\n",
       " 'et': (0.006369426751592357, 3.0),\n",
       " 'agency': (0.012738853503184714, 3.0),\n",
       " 'set': (0.012738853503184714, 3.0),\n",
       " 'new': (0.012738853503184714, 3.0),\n",
       " 'called': (0.012738853503184714, 3.0),\n",
       " 'space': (0.012738853503184714, 1.5),\n",
       " 'system': (0.012738853503184714, 3.0),\n",
       " 'sls': (0.012738853503184714, 3.0),\n",
       " 'powerful': (0.006369426751592357, 3.0),\n",
       " 'ever': (0.006369426751592357, 1.5),\n",
       " 'built': (0.006369426751592357, 3.0),\n",
       " 'uncrewed': (0.006369426751592357, 3.0),\n",
       " 'mission': (0.01910828025477707, 3.0),\n",
       " 'head': (0.012738853503184714, 3.0),\n",
       " 'toward': (0.006369426751592357, 3.0),\n",
       " 'complete': (0.006369426751592357, 3.0),\n",
       " 'one': (0.006369426751592357, 1.5),\n",
       " 'half': (0.006369426751592357, 3.0),\n",
       " 'orbits': (0.006369426751592357, 3.0),\n",
       " 'successful': (0.006369426751592357, 3.0),\n",
       " 'voyage': (0.006369426751592357, 3.0),\n",
       " 'stage': (0.006369426751592357, 3.0),\n",
       " 'crewed': (0.006369426751592357, 3.0),\n",
       " 'flyby': (0.006369426751592357, 3.0),\n",
       " 'teeing': (0.006369426751592357, 3.0),\n",
       " 'potential': (0.006369426751592357, 3.0),\n",
       " 'landing': (0.006369426751592357, 3.0),\n",
       " 'soon': (0.006369426751592357, 3.0),\n",
       " 'following': (0.006369426751592357, 3.0),\n",
       " 'year': (0.006369426751592357, 1.0),\n",
       " 'science': (0.01910828025477707, 3.0),\n",
       " 'things': (0.006369426751592357, 3.0),\n",
       " 'go': (0.012738853503184714, 3.0),\n",
       " 'according': (0.006369426751592357, 1.5),\n",
       " 'plan': (0.006369426751592357, 3.0),\n",
       " 'first': (0.006369426751592357, 3.0),\n",
       " 'ordered': (0.006369426751592357, 3.0),\n",
       " 'congress': (0.006369426751592357, 3.0),\n",
       " 'hitting': (0.006369426751592357, 3.0),\n",
       " 'launchpad': (0.006369426751592357, 3.0),\n",
       " 'numerous': (0.006369426751592357, 3.0),\n",
       " 'delays': (0.006369426751592357, 3.0),\n",
       " 'billions': (0.006369426751592357, 3.0),\n",
       " 'cost': (0.006369426751592357, 3.0),\n",
       " 'overruns': (0.006369426751592357, 3.0),\n",
       " 'plus': (0.006369426751592357, 1.5),\n",
       " 'getting': (0.006369426751592357, 3.0),\n",
       " 'humans': (0.006369426751592357, 3.0),\n",
       " 'require': (0.006369426751592357, 3.0),\n",
       " 'also': (0.012738853503184714, 3.0),\n",
       " 'vehicle': (0.006369426751592357, 3.0),\n",
       " 'send': (0.006369426751592357, 3.0),\n",
       " 'capsule': (0.006369426751592357, 3.0),\n",
       " 'spacex': (0.006369426751592357, 3.0),\n",
       " 'tapped': (0.006369426751592357, 3.0),\n",
       " 'provide': (0.006369426751592357, 3.0),\n",
       " 'lander': (0.006369426751592357, 3.0),\n",
       " 'though': (0.006369426751592357, 3.0),\n",
       " 'successfully': (0.006369426751592357, 3.0),\n",
       " 'reached': (0.006369426751592357, 3.0),\n",
       " 'orbit': (0.006369426751592357, 3.0),\n",
       " 'yet': (0.006369426751592357, 3.0),\n",
       " 'would': (0.006369426751592357, 3.0),\n",
       " 'say': (0.012738853503184714, 1.5),\n",
       " 'simply': (0.006369426751592357, 3.0),\n",
       " 'hard': (0.006369426751592357, 3.0),\n",
       " 'administrator': (0.006369426751592357, 3.0),\n",
       " 'bill': (0.006369426751592357, 3.0),\n",
       " 'nelson': (0.006369426751592357, 3.0),\n",
       " 'deadpanned': (0.006369426751592357, 3.0),\n",
       " 'saturday': (0.006369426751592357, 1.5),\n",
       " 'even': (0.006369426751592357, 3.0),\n",
       " 'reasons': (0.006369426751592357, 3.0),\n",
       " 'thinks': (0.006369426751592357, 3.0),\n",
       " 'worth': (0.006369426751592357, 3.0),\n",
       " 'trouble': (0.006369426751592357, 3.0),\n",
       " 'per': (0.006369426751592357, 3.0),\n",
       " 'npr': (0.006369426751592357, 3.0),\n",
       " 'geologists': (0.006369426751592357, 3.0),\n",
       " 'parts': (0.006369426751592357, 3.0),\n",
       " 'pivotal': (0.006369426751592357, 3.0),\n",
       " 'understanding': (0.006369426751592357, 3.0),\n",
       " 'beginnings': (0.006369426751592357, 3.0),\n",
       " 'solar': (0.006369426751592357, 3.0),\n",
       " 'atmosphere': (0.006369426751592357, 3.0),\n",
       " 'flowing': (0.006369426751592357, 3.0),\n",
       " 'water': (0.006369426751592357, 3.0),\n",
       " 'erode': (0.006369426751592357, 3.0),\n",
       " 'rocks': (0.006369426751592357, 3.0),\n",
       " 'dress': (0.006369426751592357, 3.0),\n",
       " 'rehearsal': (0.006369426751592357, 3.0),\n",
       " 'mars': (0.012738853503184714, 3.0),\n",
       " 'red': (0.006369426751592357, 3.0),\n",
       " 'planet': (0.006369426751592357, 3.0),\n",
       " 'work': (0.006369426751592357, 3.0),\n",
       " 'kinks': (0.006369426751592357, 3.0),\n",
       " 'times': (0.006369426751592357, 3.0),\n",
       " 'closer': (0.006369426751592357, 3.0),\n",
       " 'earth': (0.006369426751592357, 3.0),\n",
       " 'marketing': (0.006369426751592357, 3.0),\n",
       " 'hey': (0.006369426751592357, 3.0),\n",
       " 'talking': (0.006369426751592357, 3.0),\n",
       " 'right': (0.006369426751592357, 3.0),\n",
       " 'big': (0.006369426751592357, 1.5),\n",
       " 'buzzy': (0.006369426751592357, 3.0),\n",
       " 'projects': (0.006369426751592357, 3.0),\n",
       " 'like': (0.006369426751592357, 3.0),\n",
       " 'landings': (0.006369426751592357, 3.0),\n",
       " 'could': (0.006369426751592357, 3.0),\n",
       " 'help': (0.006369426751592357, 3.0),\n",
       " 'boost': (0.006369426751592357, 3.0),\n",
       " 'reputation': (0.006369426751592357, 3.0),\n",
       " 'inspire': (0.006369426751592357, 3.0),\n",
       " 'americans': (0.006369426751592357, 3.0),\n",
       " 'pursue': (0.006369426751592357, 3.0),\n",
       " 'engineering': (0.006369426751592357, 3.0),\n",
       " 'careers': (0.006369426751592357, 3.0),\n",
       " 'final': (0.006369426751592357, 3.0),\n",
       " 'fun': (0.006369426751592357, 3.0),\n",
       " 'fact': (0.006369426751592357, 3.0),\n",
       " 'program': (0.012738853503184714, 3.0),\n",
       " 'artemis': (0.012738853503184714, 3.0),\n",
       " 'greek': (0.006369426751592357, 3.0),\n",
       " 'myth': (0.006369426751592357, 3.0),\n",
       " 'twin': (0.006369426751592357, 3.0),\n",
       " 'sister': (0.006369426751592357, 3.0),\n",
       " 'apollo': (0.006369426751592357, 3.0),\n",
       " 'name': (0.006369426751592357, 3.0),\n",
       " 'og': (0.006369426751592357, 3.0)}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('moon', 'program'), 0.00975609756097561),\n",
       " (('new', 'moon'), 0.00975609756097561),\n",
       " (('according', 'plan'), 0.004878048780487805),\n",
       " (('administrator', 'bill'), 0.004878048780487805),\n",
       " (('agency', 'also'), 0.004878048780487805),\n",
       " (('agency', 'set'), 0.004878048780487805),\n",
       " (('also', 'inspire'), 0.004878048780487805),\n",
       " (('also', 'vehicle'), 0.004878048780487805),\n",
       " (('americans', 'pursue'), 0.004878048780487805),\n",
       " (('apollo', 'name'), 0.004878048780487805),\n",
       " (('artemis', 'greek'), 0.004878048780487805),\n",
       " (('artemis', 'twin'), 0.004878048780487805),\n",
       " (('astronauts', 'back'), 0.004878048780487805),\n",
       " (('astronauts', 'capsule'), 0.004878048780487805),\n",
       " (('astronauts', 'head'), 0.004878048780487805),\n",
       " (('atmosphere', 'flowing'), 0.004878048780487805),\n",
       " (('back', 'lunar'), 0.004878048780487805),\n",
       " (('beginnings', 'solar'), 0.004878048780487805),\n",
       " (('big', 'buzzy'), 0.004878048780487805),\n",
       " (('bill', 'nelson'), 0.004878048780487805),\n",
       " (('billions', 'cost'), 0.004878048780487805),\n",
       " (('boost', 'reputation'), 0.004878048780487805),\n",
       " (('built', 'uncrewed'), 0.004878048780487805),\n",
       " (('buzzy', 'projects'), 0.004878048780487805),\n",
       " (('called', 'artemis'), 0.004878048780487805)]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space_bigram_measrues = nltk.collocations.BigramAssocMeasures()\n",
    "space_finder = BigramCollocationFinder.from_words(space_stopped)\n",
    "space_scored = space_finder.score_ngrams(space_bigram_measrues.raw_freq)\n",
    "space_scored[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization  \n",
    "***Stemming*** is a rather naive method of word tokenization that simply removes and suffixes, prefixes, conjugations, or other modifications to the word.  \n",
    "\n",
    "***Lemmatization*** is a more sophisticated method that uses known features of a word to reduce an instance of a word to its 'lemma'.  \n",
    "<img src='images/stem_vs_lem.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context-Free Grammars (CFG) & Part of Speech Tagging (POS)  \n",
    "<img src='images/levelsOfLanguage.png' width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its useful to have more than just simple corpus statistics about a word, but also vectorizable data about the *information* embedded within a word. Context-Free Grammars are a method used to generate \"part of speech\" (POS) tags which can help make vectorizing this dimension of lanugage possible. Its important to note that when discussing CFGs we are speaking in regard to *syntax* as shown above.  \n",
    "\n",
    "A parse tree is generated by applying a senteance to a CFG that has been manually generated.\n",
    "\n",
    "consider the following sentence: \n",
    "\n",
    "***\"While hunting in Africa, I shot an elephant in my pajamas. How he got into my pajamas, I don't know.\"***\n",
    "\n",
    "A grammar can be written for this sentence that provids multiple valid POS tag formats\n",
    "\n",
    "- **S -> NP VP** A sentence (S) consists of a Noun Phrase (NP) followed by a Verb Phrase (VP).  \n",
    "- **PP -> P NP A** Prepositional Phrase (PP) consists of a Preposition (P) followed by a Noun Phrase (NP)  \n",
    "- **NP -> Det N | Det N PP | 'I'** A Noun Phrase (NP) can consist of:  \n",
    "    - a Determiner (Det) followed by a Noun (N), or (as denoted by |)  \n",
    "    - a Determiner (Det) followed by a Noun (N), followed by a Prepositional Phrase (PP), or  \n",
    "    - The token 'I'.\n",
    "- **VP -> V NP | VP PP** A Verb Phrase can consist of:\n",
    "    - a Verb (V) followed by a Noun Phrase (NP) or\n",
    "    - a Verb Phrase (VP) followed by a Prepositional Phrase (PP)\n",
    "- **Det -> 'an' | 'my'** Determiners are the tokens 'an' or 'my'\n",
    "- **N -> 'elephant' | 'pajamas'** Nouns are the tokens 'elephant' or 'pajamas'\n",
    "- **V -> 'shot'** Verbs are the token 'shot'\n",
    "- **P -> 'in'** Prepositions are the token 'in'  \n",
    "\n",
    "This grammar parses this sentence as:  \n",
    "<img src='images/sentence_parse.png' width=600>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "groucho_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP | 'I'\n",
    "VP -> V NP | VP PP\n",
    "Det -> 'an' | 'my'\n",
    "N -> 'elephant' | 'pajamas'\n",
    "V -> 'shot'\n",
    "P -> 'in'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V shot) (NP (Det an) (N elephant)))\n",
      "    (PP (P in) (NP (Det my) (N pajamas)))))\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (V shot)\n",
      "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n"
     ]
    }
   ],
   "source": [
    "parser = nltk.ChartParser(groucho_grammar)\n",
    "sent = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n",
    "\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Grammar with 15 productions>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 1: write a grammar\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N PP | N | Det NP | Adj NP\n",
    "VP -> V NP | VP PP\n",
    "Det -> 'the'\n",
    "Adj -> '100m'\n",
    "N -> 'usain_bolt' | 'record' | \n",
    "V -> 'broke'\n",
    "P -> \n",
    "\"\"\")\n",
    "grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'usain_bolt broke the 100m record'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 2: define setnence\n",
    "from nltk import word_tokenize\n",
    "sent = 'usain_bolt broke the 100m record'\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['usain_bolt', 'broke', 'the', '100m', 'record']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 3: tokenize sentence\n",
    "tokenized_sent = word_tokenize(sent)\n",
    "tokenized_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.parse.chart.ChartParser at 0x27cd90a9820>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 4: fit grammar to the parser\n",
    "parser = nltk.ChartParser(grammar)\n",
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP (V broke) (NP (Det the) (NP (Adj 100m) (NP (N record))))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (V broke)\n",
      "    (NP (Det the) (N ) (PP (P ) (NP (Adj 100m) (NP (N record)))))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (VP (V broke) (NP (N )))\n",
      "    (PP (P ) (NP (Det the) (NP (Adj 100m) (NP (N record)))))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (VP (V broke) (NP (N )))\n",
      "    (PP\n",
      "      (P )\n",
      "      (NP (Det the) (N ) (PP (P ) (NP (Adj 100m) (NP (N record))))))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (VP (VP (V broke) (NP (N ))) (PP (P ) (NP (N ))))\n",
      "    (PP (P ) (NP (Det the) (NP (Adj 100m) (NP (N record)))))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (VP (VP (V broke) (NP (N ))) (PP (P ) (NP (N ))))\n",
      "    (PP\n",
      "      (P )\n",
      "      (NP (Det the) (N ) (PP (P ) (NP (Adj 100m) (NP (N record))))))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (VP (V broke) (NP (Det the) (NP (N ))))\n",
      "    (PP (P ) (NP (Adj 100m) (NP (N record))))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (VP (V broke) (NP (Det the) (N ) (PP (P ) (NP (N )))))\n",
      "    (PP (P ) (NP (Adj 100m) (NP (N record))))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (VP (VP (V broke) (NP (N ))) (PP (P ) (NP (Det the) (NP (N )))))\n",
      "    (PP (P ) (NP (Adj 100m) (NP (N record))))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (VP\n",
      "      (VP (V broke) (NP (N )))\n",
      "      (PP (P ) (NP (Det the) (N ) (PP (P ) (NP (N ))))))\n",
      "    (PP (P ) (NP (Adj 100m) (NP (N record))))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (VP\n",
      "      (VP (VP (V broke) (NP (N ))) (PP (P ) (NP (N ))))\n",
      "      (PP (P ) (NP (Det the) (NP (N )))))\n",
      "    (PP (P ) (NP (Adj 100m) (NP (N record))))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (VP\n",
      "      (VP (VP (V broke) (NP (N ))) (PP (P ) (NP (N ))))\n",
      "      (PP (P ) (NP (Det the) (N ) (PP (P ) (NP (N ))))))\n",
      "    (PP (P ) (NP (Adj 100m) (NP (N record))))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (VP (VP (V broke) (NP (Det the) (NP (N )))) (PP (P ) (NP (N ))))\n",
      "    (PP (P ) (NP (Adj 100m) (NP (N record))))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (VP\n",
      "      (VP (V broke) (NP (Det the) (N ) (PP (P ) (NP (N )))))\n",
      "      (PP (P ) (NP (N ))))\n",
      "    (PP (P ) (NP (Adj 100m) (NP (N record))))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (VP (V broke) (NP (Det the) (NP (Adj 100m) (NP (N )))))\n",
      "    (PP (P ) (NP (N record)))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (VP\n",
      "      (V broke)\n",
      "      (NP (Det the) (N ) (PP (P ) (NP (Adj 100m) (NP (N ))))))\n",
      "    (PP (P ) (NP (N record)))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (VP\n",
      "      (VP (V broke) (NP (N )))\n",
      "      (PP (P ) (NP (Det the) (NP (Adj 100m) (NP (N ))))))\n",
      "    (PP (P ) (NP (N record)))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (VP\n",
      "      (VP (V broke) (NP (N )))\n",
      "      (PP\n",
      "        (P )\n",
      "        (NP (Det the) (N ) (PP (P ) (NP (Adj 100m) (NP (N )))))))\n",
      "    (PP (P ) (NP (N record)))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (VP\n",
      "      (VP (VP (V broke) (NP (N ))) (PP (P ) (NP (N ))))\n",
      "      (PP (P ) (NP (Det the) (NP (Adj 100m) (NP (N ))))))\n",
      "    (PP (P ) (NP (N record)))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (VP\n",
      "      (VP (VP (V broke) (NP (N ))) (PP (P ) (NP (N ))))\n",
      "      (PP\n",
      "        (P )\n",
      "        (NP (Det the) (N ) (PP (P ) (NP (Adj 100m) (NP (N )))))))\n",
      "    (PP (P ) (NP (N record)))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (VP\n",
      "      (VP (V broke) (NP (Det the) (NP (N ))))\n",
      "      (PP (P ) (NP (Adj 100m) (NP (N )))))\n",
      "    (PP (P ) (NP (N record)))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (VP\n",
      "      (VP (V broke) (NP (Det the) (N ) (PP (P ) (NP (N )))))\n",
      "      (PP (P ) (NP (Adj 100m) (NP (N )))))\n",
      "    (PP (P ) (NP (N record)))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (VP\n",
      "      (VP\n",
      "        (VP (V broke) (NP (N )))\n",
      "        (PP (P ) (NP (Det the) (NP (N )))))\n",
      "      (PP (P ) (NP (Adj 100m) (NP (N )))))\n",
      "    (PP (P ) (NP (N record)))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (VP\n",
      "      (VP\n",
      "        (VP (V broke) (NP (N )))\n",
      "        (PP (P ) (NP (Det the) (N ) (PP (P ) (NP (N ))))))\n",
      "      (PP (P ) (NP (Adj 100m) (NP (N )))))\n",
      "    (PP (P ) (NP (N record)))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (VP\n",
      "      (VP\n",
      "        (VP (VP (V broke) (NP (N ))) (PP (P ) (NP (N ))))\n",
      "        (PP (P ) (NP (Det the) (NP (N )))))\n",
      "      (PP (P ) (NP (Adj 100m) (NP (N )))))\n",
      "    (PP (P ) (NP (N record)))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (VP\n",
      "      (VP\n",
      "        (VP (VP (V broke) (NP (N ))) (PP (P ) (NP (N ))))\n",
      "        (PP (P ) (NP (Det the) (N ) (PP (P ) (NP (N ))))))\n",
      "      (PP (P ) (NP (Adj 100m) (NP (N )))))\n",
      "    (PP (P ) (NP (N record)))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (VP\n",
      "      (VP\n",
      "        (VP (V broke) (NP (Det the) (NP (N ))))\n",
      "        (PP (P ) (NP (N ))))\n",
      "      (PP (P ) (NP (Adj 100m) (NP (N )))))\n",
      "    (PP (P ) (NP (N record)))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (VP\n",
      "      (VP\n",
      "        (VP (V broke) (NP (Det the) (N ) (PP (P ) (NP (N )))))\n",
      "        (PP (P ) (NP (N ))))\n",
      "      (PP (P ) (NP (Adj 100m) (NP (N )))))\n",
      "    (PP (P ) (NP (N record)))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (VP\n",
      "      (VP (V broke) (NP (Det the) (NP (Adj 100m) (NP (N )))))\n",
      "      (PP (P ) (NP (N ))))\n",
      "    (PP (P ) (NP (N record)))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (VP\n",
      "      (VP\n",
      "        (V broke)\n",
      "        (NP (Det the) (N ) (PP (P ) (NP (Adj 100m) (NP (N ))))))\n",
      "      (PP (P ) (NP (N ))))\n",
      "    (PP (P ) (NP (N record)))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (VP (V broke) (NP (Det the) (NP (Adj 100m) (NP (N record)))))\n",
      "    (PP (P ) (NP (N )))))\n",
      "(S\n",
      "  (NP (N usain_bolt))\n",
      "  (VP\n",
      "    (VP\n",
      "      (V broke)\n",
      "      (NP (Det the) (N ) (PP (P ) (NP (Adj 100m) (NP (N record))))))\n",
      "    (PP (P ) (NP (N )))))\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: transform tokens with parser\n",
    "for tree in parser.parse(tokenized_sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('usain_bolt', 'JJ'),\n",
       " ('broke', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('100m', 'CD'),\n",
       " ('record', 'NN')]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(tokenized_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NLP has become increasingly popular over the past few years, and NLP researchers have achieved very insightful insights  \n",
    "- The Natural Language Tool Kit (NLTK) is one of the most popular Python libraries for NLP  \n",
    "- Regular Expressions are an important part of NLP, which can be used for pattern matching and filtering  \n",
    "- Regular Expressions can become confusing, so make sure to use our provided cheat sheet the first few times you work with regex  \n",
    "- It is strongly recommended you take some time to use regex tester websites to ensure you understand how changing your regex pattern affects your results when working towards a correct answer!  \n",
    "- Feature Engineering is essential when working with text data, and to understand the dynamics of your text  \n",
    "- Common feature engineering techniques are removing stop words, stemming, lemmatization, and n-grams  \n",
    "- When diving deeper into grammar and linguistics, context-free grammars and part-of-speech tagging is important  \n",
    "- In this context, parse trees can help computers when dealing with ambiguous words  \n",
    "- How you clean and preprocess your data will have a major effect on the conclusions you'll be able to draw in your NLP classification problems  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('learn-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8eb6b2671ae257969e1e6d572d1bfd2dc1c23f390a7081d26c146f9b82ef978d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

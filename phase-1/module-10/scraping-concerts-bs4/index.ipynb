{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Scraping Concerts - Lab\n", "\n", "## Introduction\n", "\n", "Now that you've seen how to scrape a simple website, it's time to again practice those skills on a full-fledged site!\n", "\n", "In this lab, you'll practice your scraping skills on an online music magazine and events website called Resident Advisor.\n", "\n", "## Objectives\n", "\n", "You will be able to:\n", "* Create a full scraping pipeline that involves traversing over many pages of a website, dealing with errors and storing data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## View the Website\n", "\n", "For this lab, you'll be scraping the https://ra.co website. For reproducibility we will use the [Internet Archive](https://archive.org/) Wayback Machine to retrieve a version of this page from March 2019.\n", "\n", "Start by navigating to the events page [here](https://web.archive.org/web/20210325230938/https://ra.co/events/us/newyork?week=2019-03-30) in your browser. It should look something like this:\n", "\n", "<img src=\"images/ra_top.png\">"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Open the Inspect Element Feature\n", "\n", "Next, open the inspect element feature from your web browser in order to preview the underlying HTML associated with the page."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Write a Function to Scrape all of the Events on the Given Page\n", "\n", "The function should return a Pandas DataFrame with columns for the `Event_Name`, `Venue`, and `Number_of_Attendees`.\n", "\n", "Start by importing the relevant libraries, making a request to the relevant URL, and exploring the contents of the response with `BeautifulSoup`. Then fill in the `scrape_events` function with the relevant code."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Relevant imports"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["EVENTS_PAGE_URL = \"https://web.archive.org/web/20210326225933/https://ra.co/events/us/newyork?week=2019-03-30\"\n", "\n", "# Exploration: making the request and parsing the response\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Find the container with event listings in it\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Find a list of events by date within that container\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Extract the date (e.g. Sat, 30 Mar) from one of those containers\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Extract the name, venue, and number of attendees from one of the\n", "# events within that container\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Loop over all of the event entries, extract this information\n", "# from each, and assemble a dataframe\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Bring it all together in a function that makes the request, gets the\n", "# list of entries from the response, loops over that list to extract the\n", "# name, venue, date, and number of attendees for each event, and returns\n", "# that list of events as a dataframe\n", "\n", "def scrape_events(events_page_url):\n", "    #Your code here\n", "    df.columns = [\"Event_Name\", \"Venue\", \"Event_Date\", \"Number_of_Attendees\"]\n", "    return df"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Test out your function\n", "scrape_events(EVENTS_PAGE_URL)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Write a Function to Retrieve the URL for the Next Page\n", "\n", "As you scroll down, there should be a button labeled \"Next Week\" that will take you to the next page of events. Write code to find that button and extract the URL from it.\n", "\n", "This is a relative path, so make sure you add `https://web.archive.org` to the front to get the URL.\n", "\n", "![next page](images/ra_next.png)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Find the button, find the relative path, create the URL for the current `soup`\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Fill in this function, to take in the current page's URL and return the\n", "# next page's URL\n", "def next_page(url):\n", "    #Your code here\n", "    return next_page_url"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Test out your function\n", "next_page(EVENTS_PAGE_URL)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Scrape the Next 500 Events\n", "\n", "In other words, repeatedly call `scrape_events` and `next_page` until you have assembled a dataframe with at least 500 rows.\n", "\n", "Display the data sorted by the number of attendees, greatest to least.\n", "\n", "We recommend adding a brief `time.sleep` call between `requests.get` calls to avoid rate limiting."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your code here"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Summary \n", "\n", "Congratulations! In this lab, you successfully developed a pipeline to scrape a website for concert event information!"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 2}